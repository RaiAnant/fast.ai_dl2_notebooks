{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "dl2_fastai_lec10.ipynb",
      "version": "0.3.2",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "0XXGSbZjpoHP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "2a430a83-63a8-429a-fa66-ba278eee989b"
      },
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "\n",
        "%matplotlib inline"
      ],
      "execution_count": 137,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The autoreload extension is already loaded. To reload it, use:\n",
            "  %reload_ext autoreload\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dUKCK8DxNWHW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from pathlib import Path\n",
        "from IPython.core.debugger import set_trace\n",
        "from fastai import datasets\n",
        "import pickle, gzip, math, torch, matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "from torch import tensor\n",
        "import operator\n",
        "from torch.nn import init\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "from functools import partial\n",
        "from torch import optim\n",
        "\n",
        "MNIST_URL='http://deeplearning.net/data/mnist/mnist.pkl'\n",
        "\n",
        "\n",
        "def test(a,b,cmp,cname=None):\n",
        "    if cname is None: cname=cmp.__name__\n",
        "    assert cmp(a,b),f\"{cname}:\\n{a}\\n{b}\"\n",
        "\n",
        "def test_eq(a,b): test(a,b,operator.eq,'==')\n",
        "\n",
        "def near(a,b): return torch.allclose(a, b, rtol=1e-3, atol=1e-5)\n",
        "\n",
        "def test_near(a,b): test(a,b,near)  \n",
        "  \n",
        "def get_data():\n",
        "    path = datasets.download_data(MNIST_URL, ext='.gz')\n",
        "    with gzip.open(path, 'rb') as f:\n",
        "        ((x_train, y_train), (x_valid, y_valid), _) = pickle.load(f, encoding='latin-1')\n",
        "    return map(tensor, (x_train,y_train,x_valid,y_valid))\n",
        "\n",
        "def normalize(x, m, s): return (x-m)/s\n",
        "\n",
        "def test_near_zero(a,tol=1e-3): assert a.abs()<tol, f\"Near zero: {a}\"\n",
        "  \n",
        "def mse(output, targ): return (output.squeeze(-1) - targ).pow(2).mean()\n",
        "\n",
        "class Dataset():\n",
        "    def __init__(self, x, y): self.x,self.y = x,y\n",
        "    def __len__(self): return len(self.x)\n",
        "    def __getitem__(self,i): return self.x[i],self.y[i]\n",
        "\n",
        "from torch.utils.data import DataLoader, SequentialSampler, RandomSampler\n",
        "\n",
        "def get_dls(train_ds, valid_ds, bs, **kwargs):\n",
        "    return (DataLoader(train_ds, batch_size=bs, shuffle=True, **kwargs),\n",
        "        DataLoader(valid_ds, batch_size=bs*2, **kwargs))\n",
        "    \n",
        "class DataBunch():\n",
        "    def __init__(self, train_dl, valid_dl, c=None):\n",
        "        self.train_dl,self.valid_dl,self.c = train_dl,valid_dl,c\n",
        "        \n",
        "    @property\n",
        "    def train_ds(self): return self.train_dl.dataset\n",
        "        \n",
        "    @property\n",
        "    def valid_ds(self): return self.valid_dl.dataset\n",
        "\n",
        "def get_model(data, lr=0.5, nh=50):\n",
        "    m = data.train_ds.x.shape[1]\n",
        "    model = nn.Sequential(nn.Linear(m,nh), nn.ReLU(), nn.Linear(nh,data.c))\n",
        "    return model, optim.SGD(model.parameters(), lr=lr)\n",
        "\n",
        "class Learner():\n",
        "    def __init__(self, model, opt, loss_func, data):\n",
        "        self.model,self.opt,self.loss_func,self.data = model,opt,loss_func,data\n",
        "\n",
        "import re\n",
        "\n",
        "_camel_re1 = re.compile('(.)([A-Z][a-z]+)')\n",
        "_camel_re2 = re.compile('([a-z0-9])([A-Z])')\n",
        "\n",
        "def camel2snake(name):\n",
        "    s1 = re.sub(_camel_re1, r'\\1_\\2', name)\n",
        "    return re.sub(_camel_re2, r'\\1_\\2', s1).lower()\n",
        "  \n",
        "class Callback():\n",
        "    _order=0\n",
        "    def set_runner(self, run): self.run = run\n",
        "    def __getattr__(self, k): return getattr(self.run, k)\n",
        "    \n",
        "    @property \n",
        "    def name(self):\n",
        "      name = re.sub(r'Callback$','', self.__class__.__name__)\n",
        "      return camel2snake(name or 'callback')\n",
        "\n",
        "from typing import *\n",
        "\n",
        "def listify(o):\n",
        "    if o is None: return []\n",
        "    if isinstance(o, list): return o\n",
        "    if isinstance(o, Iterable): return list(o)\n",
        "    return [o]\n",
        "\n",
        "class AvgStats():\n",
        "  def __init__(self, metrics, in_train): self.metrics, self.in_train = listify(metrics), in_train\n",
        "    \n",
        "  def reset(self):\n",
        "    self.tot_loss, self.count = 0., 0\n",
        "    self.tot_mets = [0.]*len(self.metrics)\n",
        "    \n",
        "  @property\n",
        "  def all_stats(self): return [self.tot_loss.item()] + self.tot_mets\n",
        "  \n",
        "  @property\n",
        "  def avg_stats(self): return [o/self.count for o in self.all_stats]\n",
        "  \n",
        "  def __repr__(self):\n",
        "    if not self.count: return \"\"\n",
        "    return f\"{'train' if self.in_train else 'valid'}: {self.avg_stats}\"\n",
        "  \n",
        "  def accumulate(self, run):\n",
        "    bn = run.xb.shape[0]\n",
        "    self.tot_loss += run.loss*bn\n",
        "    self.count += bn\n",
        "    for i,m in enumerate(self.metrics):\n",
        "      self.tot_mets[i] += m(run.pred, run.yb)*bn\n",
        "      \n",
        "class AvgStatsCallback(Callback):\n",
        "    def __init__(self, metrics):\n",
        "        self.train_stats,self.valid_stats = AvgStats(metrics,True),AvgStats(metrics,False)\n",
        "        \n",
        "    def begin_epoch(self):\n",
        "        self.train_stats.reset()\n",
        "        self.valid_stats.reset()\n",
        "        \n",
        "    def after_loss(self):\n",
        "        stats = self.train_stats if self.in_train else self.valid_stats\n",
        "        with torch.no_grad(): stats.accumulate(self.run)\n",
        "    \n",
        "    def after_epoch(self):\n",
        "        print(self.train_stats)\n",
        "        print(self.valid_stats)\n",
        "\n",
        "def create_learner(model_func, loss_func, data):\n",
        "    return Learner(*model_func(data), loss_func, data)\n",
        "\n",
        "def get_model_func(lr=0.5): return partial(get_model, lr=lr)\n",
        "\n",
        "class Recorder(Callback):\n",
        "    def begin_fit(self): self.lrs,self.losses = [],[]\n",
        "\n",
        "    def after_batch(self):\n",
        "        if not self.in_train: return\n",
        "        self.lrs.append(self.opt.param_groups[-1]['lr'])\n",
        "        self.losses.append(self.loss.detach().cpu())        \n",
        "\n",
        "    def plot_lr  (self): plt.plot(self.lrs)\n",
        "    def plot_loss(self): plt.plot(self.losses)\n",
        "\n",
        "class ParamScheduler(Callback):\n",
        "    _order=1\n",
        "    def __init__(self, pname, sched_func): self.pname,self.sched_func = pname,sched_func\n",
        "\n",
        "    def set_param(self):\n",
        "        for pg in self.opt.param_groups:\n",
        "            pg[self.pname] = self.sched_func(self.n_epochs/self.epochs)\n",
        "            \n",
        "    def begin_batch(self): \n",
        "        if self.in_train: self.set_param()\n",
        "\n",
        "def annealer(f):\n",
        "    def _inner(start, end): return partial(f, start, end)\n",
        "    return _inner\n",
        "\n",
        "@annealer\n",
        "def sched_lin(start, end, pos): return start + pos*(end-start)\n",
        "\n",
        "@annealer\n",
        "def sched_cos(start, end, pos): return start + (1 + math.cos(math.pi*(1-pos))) * (end-start) / 2\n",
        "@annealer\n",
        "def sched_no(start, end, pos):  return start\n",
        "@annealer\n",
        "def sched_exp(start, end, pos): return start * (end/start) ** pos\n",
        "\n",
        "def accuracy(out, yb): return (torch.argmax(out, dim=1)==yb).float().mean()\n",
        "\n",
        "#This monkey-patch is there to be able to plot tensors\n",
        "torch.Tensor.ndim = property(lambda x: len(x.shape))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gHkc9Fptnms5",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "# Early stopping\n",
        "\n",
        "**Better callback cancellation**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xKk_hugJrusw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_train,y_train,x_valid,y_valid = get_data()\n",
        "train_ds,valid_ds = Dataset(x_train, y_train),Dataset(x_valid, y_valid)\n",
        "nh,bs = 50,512\n",
        "c = y_train.max().item()+1\n",
        "loss_func = F.cross_entropy"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o8tnWSpisRue",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data = DataBunch(*get_dls(train_ds, valid_ds, bs), c)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "67oGqZfalwQ_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Callback():\n",
        "    _order=0\n",
        "    def set_runner(self, run): self.run=run\n",
        "    def __getattr__(self, k): return getattr(self.run, k)\n",
        "    \n",
        "    @property\n",
        "    def name(self):\n",
        "        name = re.sub(r'Callback$', '', self.__class__.__name__)\n",
        "        return camel2snake(name or 'callback')\n",
        "    \n",
        "    def __call__(self, cb_name):\n",
        "        f = getattr(self, cb_name, None)\n",
        "        if f and f(): return True\n",
        "        return False\n",
        "\n",
        "class TrainEvalCallback(Callback):\n",
        "    def begin_fit(self):\n",
        "        self.run.n_epochs=0.\n",
        "        self.run.n_iter=0\n",
        "    \n",
        "    def after_batch(self):\n",
        "        if not self.in_train: return\n",
        "        self.run.n_epochs += 1./self.iters\n",
        "        self.run.n_iter   += 1\n",
        "        \n",
        "    def begin_epoch(self):\n",
        "        self.run.n_epochs=self.epoch\n",
        "        self.model.train()\n",
        "        self.run.in_train=True\n",
        "\n",
        "    def begin_validate(self):\n",
        "        self.model.eval()\n",
        "        self.run.in_train=False\n",
        "\n",
        "class CancelTrainException(Exception): pass\n",
        "class CancelEpochException(Exception): pass\n",
        "class CancelBatchException(Exception): pass"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HaADBsPBoWIW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Runner():\n",
        "    def __init__(self, cbs=None, cb_funcs=None):\n",
        "        cbs = listify(cbs)\n",
        "        for cbf in listify(cb_funcs):\n",
        "            cb = cbf()\n",
        "            setattr(self, cb.name, cb)\n",
        "            cbs.append(cb)\n",
        "        self.stop,self.cbs = False,[TrainEvalCallback()]+cbs\n",
        "\n",
        "    @property\n",
        "    def opt(self):       return self.learn.opt\n",
        "    @property\n",
        "    def model(self):     return self.learn.model\n",
        "    @property\n",
        "    def loss_func(self): return self.learn.loss_func\n",
        "    @property\n",
        "    def data(self):      return self.learn.data\n",
        "\n",
        "    def one_batch(self, xb, yb):\n",
        "        try:\n",
        "            self.xb,self.yb = xb,yb\n",
        "            self('begin_batch')\n",
        "            self.pred = self.model(self.xb)\n",
        "            self('after_pred')\n",
        "            self.loss = self.loss_func(self.pred, self.yb)\n",
        "            self('after_loss')\n",
        "            if not self.in_train: return\n",
        "            self.loss.backward()\n",
        "            self('after_backward')\n",
        "            self.opt.step()\n",
        "            self('after_step')\n",
        "            self.opt.zero_grad()\n",
        "        except CancelBatchException: self('after_cancel_batch')\n",
        "        finally: self('after_batch')\n",
        "\n",
        "    def all_batches(self, dl):\n",
        "        self.iters = len(dl)\n",
        "        try:\n",
        "            for xb,yb in dl: self.one_batch(xb, yb)\n",
        "        except CancelEpochException: self('after_cancel_epoch')\n",
        "\n",
        "    def fit(self, epochs, learn):\n",
        "        self.epochs,self.learn,self.loss = epochs,learn,tensor(0.)\n",
        "\n",
        "        try:\n",
        "            for cb in self.cbs: cb.set_runner(self)\n",
        "            self('begin_fit')\n",
        "            for epoch in range(epochs):\n",
        "                self.epoch = epoch\n",
        "                if not self('begin_epoch'): self.all_batches(self.data.train_dl)\n",
        "\n",
        "                with torch.no_grad(): \n",
        "                    if not self('begin_validate'): self.all_batches(self.data.valid_dl)\n",
        "                self('after_epoch')\n",
        "            \n",
        "        except CancelTrainException: self('after_cancel_train')\n",
        "        finally:\n",
        "            self('after_fit')\n",
        "            self.learn = None\n",
        "\n",
        "    def __call__(self, cb_name):\n",
        "        res = False\n",
        "        for cb in sorted(self.cbs, key=lambda x: x._order): res = cb(cb_name) and res\n",
        "        return res"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "23ztNMeookY5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "learn = create_learner(get_model, loss_func, data)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n5EXRgWauPCZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class TestCallback(Callback):\n",
        "    _order=1\n",
        "    def after_step(self):\n",
        "        print(self.n_iter)\n",
        "        if self.n_iter>=10: raise CancelTrainException()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mP2w2NpLuCnO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "run = Runner(cb_funcs=TestCallback)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "80qJhGbUuEOG",
        "colab_type": "code",
        "outputId": "f691015d-4d16-400d-8942-2d2c0c81caca",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 217
        }
      },
      "source": [
        "run.fit(3, learn)"
      ],
      "execution_count": 146,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "38sCbQg_uHCC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#export\n",
        "class AvgStatsCallback(Callback):\n",
        "    def __init__(self, metrics):\n",
        "        self.train_stats,self.valid_stats = AvgStats(metrics,True),AvgStats(metrics,False)\n",
        "        \n",
        "    def begin_epoch(self):\n",
        "        self.train_stats.reset()\n",
        "        self.valid_stats.reset()\n",
        "        \n",
        "    def after_loss(self):\n",
        "        stats = self.train_stats if self.in_train else self.valid_stats\n",
        "        with torch.no_grad(): stats.accumulate(self.run)\n",
        "    \n",
        "    def after_epoch(self):\n",
        "        print(self.train_stats)\n",
        "        print(self.valid_stats)\n",
        "\n",
        "class Recorder(Callback):\n",
        "    def begin_fit(self):\n",
        "        self.lrs = [[] for _ in self.opt.param_groups] #DOUBT\n",
        "        self.losses = []\n",
        "\n",
        "    def after_batch(self):\n",
        "        if not self.in_train: return\n",
        "        for pg,lr in zip(self.opt.param_groups,self.lrs): lr.append(pg['lr'])\n",
        "            \n",
        "        self.losses.append(self.loss.detach().cpu())#doubt        \n",
        "\n",
        "    def plot_lr  (self, pgid=-1): plt.plot(self.lrs[pgid])\n",
        "    def plot_loss(self, skip_last=0): plt.plot(self.losses[:len(self.losses)-skip_last])\n",
        "        \n",
        "    def plot(self, skip_last=0, pgid=-1):\n",
        "        losses = [o.item() for o in self.losses]\n",
        "        lrs    = self.lrs[pgid]\n",
        "        n = len(losses)-skip_last\n",
        "        plt.xscale('log')\n",
        "        plt.plot(lrs[:n], losses[:n])\n",
        "\n",
        "class ParamScheduler(Callback):\n",
        "    _order=1\n",
        "    def __init__(self, pname, sched_funcs): self.pname,self.sched_funcs = pname,sched_funcs\n",
        "        \n",
        "    def begin_fit(self):\n",
        "        if not isinstance(self.sched_funcs, (list,tuple)):\n",
        "            self.sched_funcs = [self.sched_funcs] * len(self.opt.param_groups)\n",
        "\n",
        "    def set_param(self):\n",
        "        assert len(self.opt.param_groups)==len(self.sched_funcs)\n",
        "        for pg,f in zip(self.opt.param_groups,self.sched_funcs):\n",
        "            pg[self.pname] = f(self.n_epochs/self.epochs)\n",
        "            \n",
        "    def begin_batch(self): \n",
        "        if self.in_train: self.set_param()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2jGdZ17rusQT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class LR_Find(Callback):\n",
        "    _order=1\n",
        "    def __init__(self, max_iter=100, min_lr=1e-6, max_lr=10):\n",
        "        self.max_iter,self.min_lr,self.max_lr = max_iter,min_lr,max_lr\n",
        "        self.best_loss = 1e9\n",
        "        \n",
        "    def begin_batch(self): \n",
        "        if not self.in_train: return\n",
        "        pos = self.n_iter/self.max_iter\n",
        "        lr = self.min_lr  * (self.max_lr/self.min_lr) ** pos\n",
        "        for pg in self.opt.param_groups: pg['lr'] = lr\n",
        "            \n",
        "    def after_step(self):\n",
        "        if self.n_iter>=self.max_iter or self.loss>self.best_loss*10:\n",
        "            raise CancelTrainException()\n",
        "        if self.loss < self.best_loss: self.best_loss = self.loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xn4tEei5uxpj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "learn = create_learner(get_model, loss_func, data)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "11h9d7pTGks5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "run = Runner(cb_funcs= Recorder)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hR3NNevxGrEl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "run.fit(2, learn)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ncAN5ArwGtce",
        "colab_type": "code",
        "outputId": "0b72a1c6-e9c4-4e50-9989-d21f6468ffd9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 274
        }
      },
      "source": [
        "run.recorder.plot(skip_last=5)"
      ],
      "execution_count": 152,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAEACAYAAABI5zaHAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ij1I01HJGx3h",
        "colab_type": "code",
        "outputId": "264457af-b55f-4ff8-9f60-71835eef820c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        }
      },
      "source": [
        "run.recorder.plot_lr()"
      ],
      "execution_count": 153,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAD8CAYAAACb4nSYAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nq6OcNVImAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w1ooWwT_G7zH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "create_learner??"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Md21OOX9IHNH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "torch.set_num_threads(2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o1NTQpC5fxBq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_train,y_train,x_valid,y_valid = get_data()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gBxsmOypfzb7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def normalize_to(train, valid):\n",
        "    m,s = train.mean(),train.std()\n",
        "    return normalize(train, m, s), normalize(valid, m, s)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BqoDXCngf2xL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_train,x_valid = normalize_to(x_train,x_valid)\n",
        "train_ds,valid_ds = Dataset(x_train, y_train),Dataset(x_valid, y_valid)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QOo3k0glf64y",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "a0f60afa-f6ce-4921-92a3-f457839e57b3"
      },
      "source": [
        "x_train.mean(),x_train.std()"
      ],
      "execution_count": 159,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor(3.0614e-05), tensor(1.))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 159
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mkoqcvmzf-PT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "nh,bs = 50,512\n",
        "c = y_train.max().item()+1\n",
        "loss_func = F.cross_entropy\n",
        "\n",
        "data = DataBunch(*get_dls(train_ds, valid_ds, bs), c)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VfTS2g1dgB9i",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#export\n",
        "class Lambda(nn.Module):\n",
        "    def __init__(self, func):\n",
        "        super().__init__()\n",
        "        self.func = func\n",
        "\n",
        "    def forward(self, x): return self.func(x)\n",
        "\n",
        "def flatten(x):      return x.view(x.shape[0], -1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sT4LdFAGgHGz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def mnist_resize(x): return x.view(-1, 1, 28, 28)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uESQ9IwFgKVj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_cnn_model(data):\n",
        "    return nn.Sequential(\n",
        "        Lambda(mnist_resize),\n",
        "        nn.Conv2d( 1, 8, 5, padding=2,stride=2), nn.ReLU(), #14\n",
        "        nn.Conv2d( 8,16, 3, padding=1,stride=2), nn.ReLU(), # 7\n",
        "        nn.Conv2d(16,32, 3, padding=1,stride=2), nn.ReLU(), # 4\n",
        "        nn.Conv2d(32,32, 3, padding=1,stride=2), nn.ReLU(), # 2\n",
        "        nn.AdaptiveAvgPool2d(1),\n",
        "        Lambda(flatten),\n",
        "        nn.Linear(32,data.c)\n",
        "    )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PxmnNYK2gNLQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = get_cnn_model(data)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IHN-2QQ4gnQL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cbfs = [Recorder, partial(AvgStatsCallback,accuracy)]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KgN5XBDrgtDM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "opt = optim.SGD(model.parameters(), lr=0.4)\n",
        "learn = Learner(model, opt, loss_func, data)\n",
        "run = Runner(cb_funcs=cbfs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2mAhbKUThPEL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        },
        "outputId": "4fdf1385-4112-4b71-b1c4-8ce20938a234"
      },
      "source": [
        "%time run.fit(1, learn)"
      ],
      "execution_count": 167,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train: [1.6279734375, tensor(0.4530)]\n",
            "valid: [0.724287255859375, tensor(0.7688)]\n",
            "CPU times: user 5.37 s, sys: 379 ms, total: 5.75 s\n",
            "Wall time: 4.91 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DLiEOf5ClvYc",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "# CUDA\n",
        "This took a long time to run, so it's time to use a GPU. A simple Callback can make sure the model, inputs and targets are all on the same device."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CCNFySGHhaar",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Somewhat more flexible way\n",
        "device = torch.device('cuda',0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AwbCs_axjfQg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class CudaCallback(Callback):\n",
        "    def __init__(self,device): self.device=device\n",
        "    def begin_fit(self): self.model.to(self.device)\n",
        "    def begin_batch(self): self.run.xb,self.run.yb = self.xb.to(self.device),self.yb.to(self.device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ld5RJEE8jllb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Somewhat less flexible, but quite convenient\n",
        "torch.cuda.set_device(device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9--7QiB5l_0j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#export\n",
        "class CudaCallback(Callback):\n",
        "    def begin_fit(self): self.model.cuda()\n",
        "    def begin_batch(self): self.run.xb,self.run.yb = self.xb.cuda(),self.yb.cuda()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HeCMoVZjmH_j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cbfs.append(CudaCallback)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HBWO3ZRZoF6D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = get_cnn_model(data)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NK_6bPYToHcz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "opt = optim.SGD(model.parameters(), lr=0.4)\n",
        "learn = Learner(model, opt, loss_func, data)\n",
        "run = Runner(cb_funcs=cbfs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ViXxxraRoI-r",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 163
        },
        "outputId": "843f91fb-541b-42a7-d426-aab64c00c31c"
      },
      "source": [
        "%time run.fit(3, learn)"
      ],
      "execution_count": 178,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train: [2.16308703125, tensor(0.2351, device='cuda:0')]\n",
            "valid: [1.6107638671875, tensor(0.4439, device='cuda:0')]\n",
            "train: [0.582358828125, tensor(0.8163, device='cuda:0')]\n",
            "valid: [0.228528564453125, tensor(0.9325, device='cuda:0')]\n",
            "train: [0.2214387109375, tensor(0.9327, device='cuda:0')]\n",
            "valid: [0.18489920654296876, tensor(0.9426, device='cuda:0')]\n",
            "CPU times: user 5.31 s, sys: 1.43 s, total: 6.74 s\n",
            "Wall time: 9.71 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nbq6NtPMoLz6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}