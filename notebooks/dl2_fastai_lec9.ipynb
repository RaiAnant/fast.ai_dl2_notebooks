{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "dl2_fastai_lec9.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "dUKCK8DxNWHW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from pathlib import Path\n",
        "from IPython.core.debugger import set_trace\n",
        "from fastai import datasets\n",
        "import pickle, gzip, math, torch, matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "from torch import tensor\n",
        "import operator\n",
        "from torch.nn import init\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "MNIST_URL='http://deeplearning.net/data/mnist/mnist.pkl'\n",
        "\n",
        "\n",
        "def test(a,b,cmp,cname=None):\n",
        "    if cname is None: cname=cmp.__name__\n",
        "    assert cmp(a,b),f\"{cname}:\\n{a}\\n{b}\"\n",
        "\n",
        "def test_eq(a,b): test(a,b,operator.eq,'==')\n",
        "\n",
        "def near(a,b): return torch.allclose(a, b, rtol=1e-3, atol=1e-5)\n",
        "\n",
        "def test_near(a,b): test(a,b,near)  \n",
        "  \n",
        "def get_data():\n",
        "    path = datasets.download_data(MNIST_URL, ext='.gz')\n",
        "    with gzip.open(path, 'rb') as f:\n",
        "        ((x_train, y_train), (x_valid, y_valid), _) = pickle.load(f, encoding='latin-1')\n",
        "    return map(tensor, (x_train,y_train,x_valid,y_valid))\n",
        "\n",
        "def normalize(x, m, s): return (x-m)/s\n",
        "\n",
        "def test_near_zero(a,tol=1e-3): assert a.abs()<tol, f\"Near zero: {a}\"\n",
        "  \n",
        "def mse(output, targ): return (output.squeeze(-1) - targ).pow(2).mean()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hjZ3pyglN3wd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "\n",
        "%matplotlib inline"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cixl9tCxNu4s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "mpl.rcParams['image.cmap'] = 'gray'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BylmiT_bN0Vq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_train,y_train,x_valid,y_valid = get_data()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J8PmstIeXe3r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "n,m = x_train.shape\n",
        "c = y_train.max()+1\n",
        "nh = 50"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mlBXKvYuXjro",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Model(nn.Module):\n",
        "    def __init__(self, n_in, nh, n_out):\n",
        "        super().__init__()\n",
        "        self.layers = [nn.Linear(n_in,nh), nn.ReLU(), nn.Linear(nh,n_out)]\n",
        "        \n",
        "    def __call__(self, x):\n",
        "        for l in self.layers: x = l(x)\n",
        "        return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kfYdWsNhXoNm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = Model(m, nh, 10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GxuoB3PFXsUE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pred = model(x_train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5kugE0F2YEGF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "F??"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sgeZxLOGXt54",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def log_softmax(x): return (x.exp()/(x.exp().sum(-1,keepdim = True))).log() # why -1 and why keepdim?"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uKYjaO6zYyjw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test = torch.randn(5,10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Y75enF2Y3FC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test,test.sum(0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AobgzTLdY-Qs",
        "colab_type": "code",
        "outputId": "fba44be0-56db-44a2-c94f-ee1ec012408a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "test.sum(0,keepdim = True)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 0.2827,  2.9993,  2.8277,  1.6860,  1.0868, -2.9586,  3.2560, -0.8617,\n",
              "          0.5445, -0.7194]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lfly2gXIZRpL",
        "colab_type": "code",
        "outputId": "a088e928-85ff-4a06-eedf-d2fac65fed30",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 108
        }
      },
      "source": [
        "test.sum(-1),test.sum(-1,keepdim = True)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([ 6.6204,  0.3666,  5.1522, -0.7805, -3.2153]), tensor([[ 6.6204],\n",
              "         [ 0.3666],\n",
              "         [ 5.1522],\n",
              "         [-0.7805],\n",
              "         [-3.2153]]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K-1AN6OkZYBB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sm_pred = log_softmax(pred)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o6fvB1ufZv_v",
        "colab_type": "text"
      },
      "source": [
        "The cross entropy loss for some target $x$ and some prediction $p(x)$ is given by:\n",
        "\n",
        "$$ -\\sum x\\, \\log p(x) $$\n",
        "But since our $x$s are 1-hot encoded, this can be rewritten as $-\\log(p_{i})$ where i is the index of the desired target.\n",
        "\n",
        "This can be done using numpy-style integer array indexing. Note that PyTorch supports all the tricks in the advanced indexing methods discussed in that link."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qWJaMXa_ZvOn",
        "colab_type": "code",
        "outputId": "096e38d1-22a6-4c5e-c0e7-896d35ab2a59",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "y_train[:3]"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([5, 0, 4])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IqvEfPLmsblP",
        "colab_type": "code",
        "outputId": "02e55175-5dad-46b7-c5e4-55dc1b889372",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "sm_pred.shape"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([50000, 10])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c9TE4nlBwEDR",
        "colab_type": "text"
      },
      "source": [
        "Indexing ways"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rTCCbuIBvkZy",
        "colab_type": "code",
        "outputId": "d91ec859-0ca2-4cf6-bb09-f21cdc6fb0e6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 126
        }
      },
      "source": [
        "sm_pred[[1,2,3]]"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-2.1611, -2.2920, -2.2587, -2.2874, -2.2793, -2.2895, -2.2893, -2.2790,\n",
              "         -2.4614, -2.4651],\n",
              "        [-2.2064, -2.3798, -2.2345, -2.3736, -2.2850, -2.2756, -2.2323, -2.2730,\n",
              "         -2.4147, -2.3756],\n",
              "        [-2.2164, -2.2673, -2.2209, -2.3737, -2.2440, -2.3363, -2.2193, -2.3827,\n",
              "         -2.3701, -2.4237]], grad_fn=<IndexBackward>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4sN11oTRxPyT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LAkqKTqTwBZR",
        "colab_type": "code",
        "outputId": "5f176e9f-44d6-440f-e7b6-881660d8c3bf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "sm_pred[[0,1,2],[5, 0, 4]]"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([-2.2690, -2.1611, -2.2850], grad_fn=<IndexBackward>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RwRBkuNzwOp6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def nll(input, target): return -input[range(target.shape[0]),target].mean()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rpCLAT_JUtOS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "loss = nll(sm_pred, y_train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HdgJnTOAxFQl",
        "colab_type": "code",
        "outputId": "f10b7a6d-506a-49d7-94f2-5d15ced3749d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "loss"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(2.2990, grad_fn=<NegBackward>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BPa0OkoVyBhZ",
        "colab_type": "text"
      },
      "source": [
        "Note that the formula\n",
        "\n",
        "$$\\log \\left ( \\frac{a}{b} \\right ) = \\log(a) - \\log(b)$$\n",
        "gives a simplification when we compute the log softmax, which was previously defined as (x.exp()/(x.exp().sum(-1,keepdim=True))).log()"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "737QMBWCx8qI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def log_softmax(x): return x - x.exp().sum(-1,keepdim=True).log()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DR5DjTD1yIWt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_near(nll(log_softmax(pred), y_train), loss)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G4lUz7SayuVe",
        "colab_type": "text"
      },
      "source": [
        "Then, there is a way to compute the log of the sum of exponentials in a more stable way, called the LogSumExp trick. The idea is to use the following formula:\n",
        "\n",
        "$$\\log \\left ( \\sum_{j=1}^{n} e^{x_{j}} \\right ) = \\log \\left ( e^{a} \\sum_{j=1}^{n} e^{x_{j}-a} \\right ) = a + \\log \\left ( \\sum_{j=1}^{n} e^{x_{j}-a} \\right )$$\n",
        "where a is the maximum of the $x_{j}$."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NFWWFnrsyLZx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def logsumexp(x):\n",
        "    m = x.max(-1)[0]\n",
        "    return m + (x-m[:,None]).exp().sum(-1).log()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k7OwMWv82iDe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_near(logsumexp(pred), pred.logsumexp(-1))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NAL6uLft2nmm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def log_softmax(x): return x - x.logsumexp(-1,keepdim=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vr7VNvjG2t3Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_near(nll(log_softmax(pred), y_train), loss)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-4SlrK3c2xcI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#PyTorch's implementation.\n",
        "test_near(F.nll_loss(F.log_softmax(pred, -1), y_train), loss)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WM3utHEI22Cm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#In PyTorch, F.log_softmax and F.nll_loss are combined in one optimized function, F.cross_entropy\n",
        "test_near(F.cross_entropy(pred, y_train), loss)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "piDX1oHq3Eqk",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "Basic training loop\n",
        "Basically the training loop repeats over the following steps:\n",
        "\n",
        "1.get the output of the model on a batch of inputs\n",
        "\n",
        "2.compare the output to the labels we have and compute a loss\n",
        "\n",
        "3.calculate the gradients of the loss with respect to every parameter of the model\n",
        "\n",
        "4.update said parameters with those gradients to make them a little bit better"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lb0jpVha3CJB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "loss_func = F.cross_entropy"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8cOoS8lu3zzb",
        "colab_type": "code",
        "outputId": "d723622f-dc46-4c2d-8b8d-8a7330c3415e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "source": [
        "temp = tensor([[0,1,0],[1,0,0],[1,0,0]]).float()\n",
        "temp"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0., 1., 0.],\n",
              "        [1., 0., 0.],\n",
              "        [1., 0., 0.]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i1BFUSnJ45kv",
        "colab_type": "code",
        "outputId": "8c5c5246-9205-4f3c-dff8-8b15f23a8101",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "torch.argmax(temp,dim=1),torch.argmax(temp,dim=0)"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([1, 0, 0]), tensor([2, 0, 2]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZMtbYr1o3X5S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def accuracy(out, yb): return (torch.argmax(out,dim=1)==yb).float().mean()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "my07FKMQ5C1-",
        "colab_type": "code",
        "outputId": "92149542-394b-4fae-9f89-237d0b82e9bd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "bs = 64\n",
        "\n",
        "xb = x_train[:bs]\n",
        "preds = model(xb)\n",
        "preds[0], preds.shape"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([ 0.0228, -0.1617, -0.0242, -0.1452, -0.0643, -0.0558,  0.0065, -0.1740,\n",
              "         -0.1598, -0.1663], grad_fn=<SelectBackward>), torch.Size([64, 10]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NpAup36Q5lM0",
        "colab_type": "code",
        "outputId": "9bdd7b02-cb1b-49c4-e7c9-91451feed5c6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "yb = y_train[0:bs]\n",
        "loss_func(preds, yb)"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(2.2991, grad_fn=<NllLossBackward>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oMhIEx5B7QHw",
        "colab_type": "code",
        "outputId": "758bde35-1174-42b7-aca8-05e3c5ac56fc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "accuracy(preds, yb)"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(0.1250)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-4hZ9z0H7UcS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lr = 0.1 \n",
        "epochs = 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KAJURLbf7v4m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "n = x_train.shape[0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dHh6wWVOKHsJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "nn.Linear??"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R48uyBdh7htX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for epoch in range(epochs):\n",
        "  for batch in range((n-1)//bs +1):\n",
        "    \n",
        "    x_batch = x_train[batch*bs:(batch+1)*bs]\n",
        "    y_batch = y_train[batch*bs:(batch+1)*bs]\n",
        "    \n",
        "    preds = model(x_batch)\n",
        "    \n",
        "    loss = loss_func(preds,y_batch)\n",
        "    \n",
        "    loss.backward()\n",
        "    \n",
        "    with torch.no_grad():\n",
        "      for l in model.layers:\n",
        "        if hasattr(l, 'weight'):\n",
        "          l.weight -= l.weight.grad*lr\n",
        "          l.bias -= l.bias.grad*lr\n",
        "          l.weight.grad.zero_()\n",
        "          l.bias.grad.zero_()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "umDCBQQZE-mC",
        "colab_type": "code",
        "outputId": "57dbb7f2-a0eb-4188-f23d-1c81b7f9398f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "loss_func(model(xb), yb), accuracy(model(xb), yb)"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor(0.3260, grad_fn=<NllLossBackward>), tensor(0.9219))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "duhixw1hUhFv",
        "colab_type": "text"
      },
      "source": [
        "Using parameters and optim\n",
        "\n",
        "Parameters\n",
        "\n",
        "Use nn.Module.__setattr__ and move relu to functional"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "og9F3S4QFKgF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Model(nn.Module):\n",
        "    def __init__(self, n_in, nh, n_out):\n",
        "        super().__init__()\n",
        "        self.l1 = nn.Linear(n_in,nh)\n",
        "        self.l2 = nn.Linear(nh,n_out)\n",
        "        \n",
        "    def __call__(self, x): return self.l2(F.relu(self.l1(x)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gyloIqjATJOV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = Model(m, nh, 10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jl97hFDyUxVA",
        "colab_type": "code",
        "outputId": "dc0e4143-df41-4b8d-c77f-e6f8d2c5e71a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "for name,l in model.named_children(): print(f\"{name}: {l}\")"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "l1: Linear(in_features=784, out_features=50, bias=True)\n",
            "l2: Linear(in_features=50, out_features=10, bias=True)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KIMtNCl6U5fH",
        "colab_type": "code",
        "outputId": "191db082-6993-476c-e808-52b406ffcb45",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        }
      },
      "source": [
        "model"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Model(\n",
              "  (l1): Linear(in_features=784, out_features=50, bias=True)\n",
              "  (l2): Linear(in_features=50, out_features=10, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LcKKrtbWVKJ5",
        "colab_type": "code",
        "outputId": "20fd753a-5b81-490d-f4ea-62be0242ac3a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "model.l1"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Linear(in_features=784, out_features=50, bias=True)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "naiMuzMWVO3A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def fit():\n",
        "    for epoch in range(epochs):\n",
        "        for i in range((n-1)//bs + 1):\n",
        "            start_i = i*bs\n",
        "            end_i = start_i+bs\n",
        "            xb = x_train[start_i:end_i]\n",
        "            yb = y_train[start_i:end_i]\n",
        "            loss = loss_func(model(xb), yb)\n",
        "\n",
        "            loss.backward()\n",
        "            with torch.no_grad():\n",
        "                for p in model.parameters(): p -= p.grad * lr\n",
        "                model.zero_grad()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GD6TOHC6aHmQ",
        "colab_type": "code",
        "outputId": "f873952c-4e57-49d4-f792-adf008b2da82",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "fit()\n",
        "loss_func(model(xb), yb), accuracy(model(xb), yb)"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor(0.3199, grad_fn=<NllLossBackward>), tensor(0.9219))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WQ7WFG6laWRb",
        "colab_type": "text"
      },
      "source": [
        "Behind the scenes, PyTorch overrides the __setattr__ function in nn.Module so that the submodules you define are properly registered as parameters of the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "84eenkEZaLSB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class DummyModule():\n",
        "    def __init__(self, n_in, nh, n_out):\n",
        "        self._modules = {}\n",
        "        self.l1 = nn.Linear(n_in,nh)\n",
        "        self.l2 = nn.Linear(nh,n_out)\n",
        "        self.n = 3\n",
        "        \n",
        "    def __setattr__(self,k,v):\n",
        "        if not k.startswith(\"_\") and hasattr(v,'parameters'): self._modules[k] = v\n",
        "        super().__setattr__(k,v)\n",
        "        \n",
        "    def __repr__(self): return f'{self._modules}'\n",
        "    \n",
        "    def parameters(self):\n",
        "        for l in self._modules.values():\n",
        "            for p in l.parameters(): yield p"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9mgsTewnaerl",
        "colab_type": "code",
        "outputId": "878fc011-e0b1-4d28-c220-6267b0e56491",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        }
      },
      "source": [
        "mdl = DummyModule(m,nh,10)\n",
        "mdl"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'l1': Linear(in_features=784, out_features=50, bias=True), 'l2': Linear(in_features=50, out_features=10, bias=True)}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KlhXNJQCa4ZP",
        "colab_type": "code",
        "outputId": "eead1c9b-1507-4582-ea98-5b4a369be6a1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        }
      },
      "source": [
        "[o.shape for o in mdl.parameters()]"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[torch.Size([50, 784]),\n",
              " torch.Size([50]),\n",
              " torch.Size([10, 50]),\n",
              " torch.Size([10])]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ANDcit_NehxT",
        "colab_type": "text"
      },
      "source": [
        "**Registering modules**\n",
        "\n",
        "We can use the original layers approach, but we have to register the modules"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W0RSQvLFbAWA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "layers = [nn.Linear(m,nh), nn.ReLU(), nn.Linear(nh,10)]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cm_eV-wPcDHG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Model(nn.Module):\n",
        "    def __init__(self, layers):\n",
        "        super().__init__()\n",
        "        self.layers = layers\n",
        "        for i,l in enumerate(self.layers): self.add_module(f'layer_{i}', l)\n",
        "        \n",
        "    def __call__(self, x):\n",
        "        for l in self.layers: x = l(x)\n",
        "        return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xS0ii6xJev8W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = Model(layers)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VQ6UoWC1e0E2",
        "colab_type": "code",
        "outputId": "d6f1dbd1-0b62-4adb-ce0d-d874b2c0e492",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 108
        }
      },
      "source": [
        "model"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Model(\n",
              "  (layer_0): Linear(in_features=784, out_features=50, bias=True)\n",
              "  (layer_1): ReLU()\n",
              "  (layer_2): Linear(in_features=50, out_features=10, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V8X3CbG7fIF-",
        "colab_type": "text"
      },
      "source": [
        "**nn.ModuleList**\n",
        "\n",
        "nn.ModuleList does this for us"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X6t3-sVPe1tr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class SequentialModel(nn.Module):\n",
        "    def __init__(self, layers):\n",
        "        super().__init__()\n",
        "        self.layers = nn.ModuleList(layers)\n",
        "        \n",
        "    def __call__(self, x):\n",
        "        for l in self.layers: x = l(x)\n",
        "        return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tHuXCPpifPdH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = SequentialModel(layers)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yr6rWTfDfRP3",
        "colab_type": "code",
        "outputId": "9c4e2bfa-1f0d-469e-b5c3-5f8526d6204e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 145
        }
      },
      "source": [
        "model"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "SequentialModel(\n",
              "  (layers): ModuleList(\n",
              "    (0): Linear(in_features=784, out_features=50, bias=True)\n",
              "    (1): ReLU()\n",
              "    (2): Linear(in_features=50, out_features=10, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zqPPz2mhfS1B",
        "colab_type": "code",
        "outputId": "bda5782e-3882-4dc4-fe89-28f01cac8f30",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "fit()\n",
        "loss_func(model(xb), yb), accuracy(model(xb), yb)"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor(0.2982, grad_fn=<NllLossBackward>), tensor(0.9219))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5v8HuspGflrO",
        "colab_type": "text"
      },
      "source": [
        "**nn.Sequential**\n",
        "\n",
        "nn.Sequential is a convenient class which does the same as the above:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mPUdXgKSfVXi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = nn.Sequential(nn.Linear(m,nh), nn.ReLU(), nn.Linear(nh,10))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bpUMMEbhiOdS",
        "colab_type": "code",
        "outputId": "62a0f888-4a5e-487d-8723-8edac93a75b2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "fit()\n",
        "loss_func(model(xb), yb), accuracy(model(xb), yb)"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor(0.2783, grad_fn=<NllLossBackward>), tensor(0.9219))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "md3eV2aLiQbz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "nn.Sequential??"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "va-5gxUziS1W",
        "colab_type": "code",
        "outputId": "f5906884-396e-4e10-f83e-54c6e2d11eae",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 108
        }
      },
      "source": [
        "model"
      ],
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Sequential(\n",
              "  (0): Linear(in_features=784, out_features=50, bias=True)\n",
              "  (1): ReLU()\n",
              "  (2): Linear(in_features=50, out_features=10, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6CYQSFwtilCU",
        "colab_type": "text"
      },
      "source": [
        "**optim**\n",
        "\n",
        "Let's replace our previous manually coded optimization step\n",
        ":\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "with torch.no_grad():\n",
        "    for p in model.parameters(): p -= p.grad * lr\n",
        "    model.zero_grad()\n",
        "```\n",
        "\n",
        "\n",
        "    \n",
        "and instead use just:\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "opt.step()\n",
        "opt.zero_grad()\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iyZR68_MijxD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Optimizer():\n",
        "  \n",
        "  def __init__(self, params, lr=0.5): self.params,self.lr=list(params),lr\n",
        "    \n",
        "  def step(self):\n",
        "    with torch.no_grad():\n",
        "      for p in self.params: p -= p.grad*lr\n",
        "        \n",
        "  def zero_grad(self):\n",
        "    for p in self.params: p.grad.data.zero_()\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KL6OeNsgjjKz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = nn.Sequential(nn.Linear(m,nh), nn.ReLU(), nn.Linear(nh,10))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zGivvEhHjmCo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "opt = Optimizer(model.parameters())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DkrOfewsjoIN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for epoch in range(epochs):\n",
        "    for i in range((n-1)//bs + 1):\n",
        "        start_i = i*bs\n",
        "        end_i = start_i+bs\n",
        "        xb = x_train[start_i:end_i]\n",
        "        yb = y_train[start_i:end_i]\n",
        "        pred = model(xb)\n",
        "        loss = loss_func(pred, yb)\n",
        "\n",
        "        loss.backward()\n",
        "        opt.step()\n",
        "        opt.zero_grad()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ir4O1DSilD7_",
        "colab_type": "code",
        "outputId": "f27d2cf9-4c2a-45c1-ddfc-5cc9a583a93b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "loss,acc = loss_func(model(xb), yb), accuracy(model(xb), yb)\n",
        "loss,acc"
      ],
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor(0.1328, grad_fn=<NllLossBackward>), tensor(1.))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 73
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "andZAjaHlM72",
        "colab_type": "text"
      },
      "source": [
        "PyTorch already provides this exact functionality in optim.SGD (it also handles stuff like momentum, which we'll look at later - except we'll be doing it in a more flexible way!)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mmH_-L2JlGv8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#export\n",
        "from torch import optim"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WiIAbOh5lV0z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "optim.SGD.step??"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CziWLb_XlYlq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_model():\n",
        "    model = nn.Sequential(nn.Linear(m,nh), nn.ReLU(), nn.Linear(nh,10))\n",
        "    return model, optim.SGD(model.parameters(), lr=lr)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KB1cYFDKrSr2",
        "colab_type": "code",
        "outputId": "a3247df0-a5bf-4a36-d2f0-50ca4705cbb1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "model,opt = get_model()\n",
        "loss_func(model(xb), yb)"
      ],
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(2.2771, grad_fn=<NllLossBackward>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 77
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aFYdlUxFrVdg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for epoch in range(epochs):\n",
        "    for i in range((n-1)//bs + 1):\n",
        "        start_i = i*bs\n",
        "        end_i = start_i+bs\n",
        "        xb = x_train[start_i:end_i]\n",
        "        yb = y_train[start_i:end_i]\n",
        "        pred = model(xb)\n",
        "        loss = loss_func(pred, yb)\n",
        "\n",
        "        loss.backward()\n",
        "        opt.step()\n",
        "        opt.zero_grad()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iskbinMercoi",
        "colab_type": "code",
        "outputId": "bb4e38ac-f90d-4203-acf0-b1d322027355",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "loss,acc = loss_func(model(xb), yb), accuracy(model(xb), yb)\n",
        "loss,acc"
      ],
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor(0.1508, grad_fn=<NllLossBackward>), tensor(1.))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 79
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F715hr6frfnO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "assert acc>0.7"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X-9d-jZarj-8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#export\n",
        "class Dataset():\n",
        "    def __init__(self, x, y): self.x,self.y = x,y\n",
        "    def __len__(self): return len(self.x)\n",
        "    def __getitem__(self,i): return self.x[i],self.y[i]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GyB_uDJPr4uo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_ds,valid_ds = Dataset(x_train,y_train), Dataset(x_valid,y_valid)\n",
        "assert len(train_ds)==len(x_train)\n",
        "assert len(valid_ds)==len(x_valid)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QHEUtkF7ssIx",
        "colab_type": "code",
        "outputId": "466a2b97-36b2-4e93-a213-6c1ac2a5f281",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 108
        }
      },
      "source": [
        "xb,yb = train_ds[0:5]\n",
        "assert xb.shape==(5,28*28)\n",
        "assert yb.shape==(5,)\n",
        "xb,yb"
      ],
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
              "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "         [0., 0., 0.,  ..., 0., 0., 0.]]), tensor([5, 0, 4, 1, 9]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 83
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cGTjmeMSs8CE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model,opt = get_model()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H1Y3QdeEs_wy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for epoch in range(epochs):\n",
        "    for i in range((n-1)//bs + 1):\n",
        "        xb,yb = train_ds[i*bs : i*bs+bs]\n",
        "        pred = model(xb)\n",
        "        loss = loss_func(pred, yb)\n",
        "\n",
        "        loss.backward()\n",
        "        opt.step()\n",
        "        opt.zero_grad()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9B52JG0ktUD7",
        "colab_type": "code",
        "outputId": "8675fc69-82b9-449e-d458-59d9e235d9c2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "loss,acc = loss_func(model(xb), yb), accuracy(model(xb), yb)\n",
        "assert acc>0.7\n",
        "loss,acc"
      ],
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor(0.1491, grad_fn=<NllLossBackward>), tensor(1.))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 86
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cKhA_HrmucNv",
        "colab_type": "text"
      },
      "source": [
        "**DataLoader**\n",
        "\n",
        "Previously, our loop iterated over batches (xb, yb) like this:\n",
        "\n",
        "\n",
        "```\n",
        "for i in range((n-1)//bs + 1):\n",
        "    xb,yb = train_ds[i*bs : i*bs+bs]\n",
        "    ...\n",
        "```\n",
        "Let's make our loop much cleaner, using a data loader:\n",
        "\n",
        "\n",
        "```\n",
        "for xb,yb in train_dl:\n",
        "```\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jdYmexnutZmI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class DataLoader():\n",
        "  def __init__(self, ds, bs): self.ds, self.bs = ds,bs\n",
        "  def __iter__(self):\n",
        "    for i in range(0, len(self.ds), self.bs): yield self.ds[i:i+self.bs] # note here yield is used"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tBgkn9YfvzD8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_dl = DataLoader(train_ds, bs)\n",
        "valid_dl = DataLoader(valid_ds, bs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_TRdEiM4v-pz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "xb,yb = next(iter(valid_dl))\n",
        "assert xb.shape==(bs,28*28)\n",
        "assert yb.shape==(bs,)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UY47_Y_hwA-4",
        "colab_type": "code",
        "outputId": "38ec02c6-ace9-4c55-ca95-120eb77c18e2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 287
        }
      },
      "source": [
        "plt.imshow(xb[0].view(28,28))\n",
        "yb[0]"
      ],
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(3)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 90
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nXGStUmRfAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zWxGsrprwMH_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model,opt = get_model()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q1wXWEfpwqPK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def fit():\n",
        "    for epoch in range(epochs):\n",
        "        for xb,yb in train_dl:\n",
        "            pred = model(xb)\n",
        "            loss = loss_func(pred, yb)\n",
        "            loss.backward()\n",
        "            opt.step()\n",
        "            opt.zero_grad()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JOwm0y4Gwr_w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "fit()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FjH_wVquws8A",
        "colab_type": "code",
        "outputId": "48f938be-6992-4241-c4fc-1527293d8f2b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "loss,acc = loss_func(model(xb), yb), accuracy(model(xb), yb)\n",
        "assert acc>0.7\n",
        "loss,acc"
      ],
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor(0.3039, grad_fn=<NllLossBackward>), tensor(0.9219))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 94
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "31zJUfXZyi5D",
        "colab_type": "text"
      },
      "source": [
        "**Random sampling**\n",
        "\n",
        "We want our training set to be in a random order, and that order should differ each iteration. But the validation set shouldn't be randomized."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zP5fyO3SwwzY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Sampler():\n",
        "    def __init__(self, ds, bs, shuffle=False):\n",
        "        self.n,self.bs,self.shuffle = len(ds),bs,shuffle\n",
        "        \n",
        "    def __iter__(self):\n",
        "        self.idxs = torch.randperm(self.n) if self.shuffle else torch.arange(self.n)\n",
        "        for i in range(0, self.n, self.bs): yield self.idxs[i:i+self.bs]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QfNdGn3CxyOJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "small_ds = Dataset(*train_ds[:10])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "43n3gwrixz-W",
        "colab_type": "code",
        "outputId": "67e28c5e-6b58-469c-b2b1-fe1ef6d011f4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "s = Sampler(small_ds,3,False)\n",
        "[o for o in s]"
      ],
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[tensor([0, 1, 2]), tensor([3, 4, 5]), tensor([6, 7, 8]), tensor([9])]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 97
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Fv-VikxyIxo",
        "colab_type": "code",
        "outputId": "26c8e516-7c65-4995-dc87-e3a510d7125b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "s = Sampler(small_ds,3,True)\n",
        "[o for o in s]"
      ],
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[tensor([4, 9, 1]), tensor([6, 8, 5]), tensor([3, 0, 2]), tensor([7])]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 98
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_nd-5nLDyO2t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def collate(b):\n",
        "    xs,ys = zip(*b)\n",
        "    return torch.stack(xs),torch.stack(ys)\n",
        "  \n",
        "class DataLoader():\n",
        "    def __init__(self, ds, sampler, collate_fn=collate):\n",
        "        self.ds,self.sampler,self.collate_fn = ds,sampler,collate_fn\n",
        "        \n",
        "    def __iter__(self):\n",
        "        for s in self.sampler: yield self.collate_fn([self.ds[i] for i in s])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PvvU2YpHyW3U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_samp = Sampler(train_ds, bs, shuffle=True)\n",
        "valid_samp = Sampler(valid_ds, bs, shuffle=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OMhmI-1WygbZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_dl = DataLoader(train_ds, sampler=train_samp, collate_fn=collate)\n",
        "valid_dl = DataLoader(valid_ds, sampler=valid_samp, collate_fn=collate)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NzLiaH4cy22a",
        "colab_type": "code",
        "outputId": "519e8392-24a4-4d0d-ee1f-18b603eff1f0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 287
        }
      },
      "source": [
        "xb,yb = next(iter(valid_dl))\n",
        "plt.imshow(xb[0].view(28,28))\n",
        "yb[0]"
      ],
      "execution_count": 102,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(3)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 102
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nXGStUmRfAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5EnONtBI3kvX",
        "colab_type": "code",
        "outputId": "15722f55-8d44-46c8-d478-8105977bef11",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 287
        }
      },
      "source": [
        "xb,yb = next(iter(train_dl))\n",
        "plt.imshow(xb[0].view(28,28))\n",
        "yb[0]"
      ],
      "execution_count": 103,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(4)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 103
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-V7654P23nm9",
        "colab_type": "code",
        "outputId": "6eb0cc59-15fc-4c2f-8ceb-6ce8c7269539",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 287
        }
      },
      "source": [
        "xb,yb = next(iter(train_dl))\n",
        "plt.imshow(xb[0].view(28,28))\n",
        "yb[0]"
      ],
      "execution_count": 104,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(3)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 104
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nPz0fZvPEiAqGAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4aBdLj6z3szx",
        "colab_type": "code",
        "outputId": "8fda5bc5-1bfa-42ef-cf0e-6550879a019f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "model,opt = get_model()\n",
        "fit()\n",
        "\n",
        "loss,acc = loss_func(model(xb), yb), accuracy(model(xb), yb)\n",
        "assert acc>0.7\n",
        "loss,acc"
      ],
      "execution_count": 105,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor(0.3622, grad_fn=<NllLossBackward>), tensor(0.8906))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 105
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wQgDoLiQ4Nui",
        "colab_type": "text"
      },
      "source": [
        "**PyTorch DataLoader**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0l_Js-wW3xRo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torch.utils.data import DataLoader, SequentialSampler, RandomSampler"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zMwt9YbJ4Tnh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_dl = DataLoader(train_ds, bs, sampler=RandomSampler(train_ds), collate_fn=collate)\n",
        "valid_dl = DataLoader(valid_ds, bs, sampler=SequentialSampler(valid_ds), collate_fn=collate)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WLGaqYjH4X72",
        "colab_type": "code",
        "outputId": "18a745a5-6a7b-4b68-dcdc-392a3fd79b01",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "model,opt = get_model()\n",
        "fit()\n",
        "loss_func(model(xb), yb), accuracy(model(xb), yb)"
      ],
      "execution_count": 108,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor(0.4605, grad_fn=<NllLossBackward>), tensor(0.8906))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 108
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fVmBiX7265SN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "DataLoader??"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qMJPWXTo4aGL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_dl = DataLoader(train_ds, bs, shuffle=True, drop_last=True)\n",
        "valid_dl = DataLoader(valid_ds, bs, shuffle=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R6JYlHP_7cSH",
        "colab_type": "text"
      },
      "source": [
        "PyTorch's defaults work fine for most things however:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gXs-_RrB4d3v",
        "colab_type": "code",
        "outputId": "d9162778-3f19-480c-de79-3613086e0411",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "model,opt = get_model()\n",
        "fit()\n",
        "\n",
        "loss,acc = loss_func(model(xb), yb), accuracy(model(xb), yb)\n",
        "assert acc>0.7\n",
        "loss,acc"
      ],
      "execution_count": 111,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor(0.4176, grad_fn=<NllLossBackward>), tensor(0.8906))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 111
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0masRS8S7kRd",
        "colab_type": "text"
      },
      "source": [
        "Note that PyTorch's DataLoader, if you pass num_workers, will use multiple threads to call your Dataset.\n",
        "\n",
        "\n",
        "\n",
        "**Validation**\n",
        "\n",
        "You always should also have a validation set, in order to identify if you are overfitting.\n",
        "\n",
        "We will calculate and print the validation loss at the end of each epoch.\n",
        "\n",
        "(Note that we always call model.train() before training, and model.eval() before inference, because these are used by layers such as nn.BatchNorm2d and nn.Dropout to ensure appropriate behaviour for these different phases.)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2NY_fgiD7Gpo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def fit(epochs, model, loss_func, opt, train_dl, valid_dl):\n",
        "    for epoch in range(epochs):\n",
        "        # Handle batchnorm / dropout\n",
        "        model.train()\n",
        "#         print(model.training)\n",
        "        for xb,yb in train_dl:\n",
        "            loss = loss_func(model(xb), yb)\n",
        "            loss.backward()\n",
        "            opt.step()\n",
        "            opt.zero_grad()\n",
        "        \n",
        "        model.eval()\n",
        "        \n",
        "        with torch.no_grad():\n",
        "          # we calculate loss and accuracy on valid set which is displayed as metric after each epoch\n",
        "          tot_loss, tot_acc = 0.,0.\n",
        "          for xb, yb in valid_dl:  \n",
        "            pred = model(xb)\n",
        "            tot_loss += loss_func(pred, yb)\n",
        "            tot_acc += accuracy(pred,yb)\n",
        "        nv = len(valid_dl)\n",
        "        print(epoch, tot_loss/nv, tot_acc/nv)\n",
        "        \n",
        "    return tot_loss/nv, tot_acc/nv\n",
        "         "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jqcZwMYo-Yep",
        "colab_type": "text"
      },
      "source": [
        "Question: Are these validation results correct if batch size varies?\n",
        "\n",
        "get_dls returns dataloaders for the training and validation sets:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bzjJu0sv8A_o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#export\n",
        "def get_dls(train_ds, valid_ds, bs, **kwargs):\n",
        "    return (DataLoader(train_ds, batch_size=bs, shuffle=True, **kwargs),\n",
        "            DataLoader(valid_ds, batch_size=bs*2, **kwargs))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j8_KHoGn8EN1",
        "colab_type": "code",
        "outputId": "ae67fd35-10f6-4ab8-b440-5c54b6ad9ebd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 108
        }
      },
      "source": [
        "train_dl,valid_dl = get_dls(train_ds, valid_ds, bs)\n",
        "model,opt = get_model()\n",
        "loss,acc = fit(5, model, loss_func, opt, train_dl, valid_dl)"
      ],
      "execution_count": 114,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 tensor(0.2712) tensor(0.9237)\n",
            "1 tensor(0.2120) tensor(0.9382)\n",
            "2 tensor(0.1842) tensor(0.9497)\n",
            "3 tensor(0.1607) tensor(0.9549)\n",
            "4 tensor(0.1558) tensor(0.9520)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O1nRVOXU-vn_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "assert acc>0.9"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iRwdYrYlViq5",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "# DataBunch/Learner"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B8WTIKYz-5oX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_train,y_train,x_valid,y_valid = get_data()\n",
        "train_ds,valid_ds = Dataset(x_train, y_train),Dataset(x_valid, y_valid)\n",
        "nh,bs = 50,64\n",
        "c = y_train.max().item()+1\n",
        "loss_func = F.cross_entropy"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p1F5tRczVwZX",
        "colab_type": "text"
      },
      "source": [
        "Factor out the connected pieces of info out of the fit() argument list\n",
        "\n",
        "fit(epochs, model, loss_func, opt, train_dl, valid_dl)\n",
        "\n",
        "Let's replace it with something that looks like this:\n",
        "\n",
        "fit(1, learn)\n",
        "\n",
        "This will allow us to tweak what's happening inside the training loop in other places of the code because the Learner object will be mutable, so changing any of its attribute elsewhere will be seen in our training loop"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x4R5hLsmWspv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "DataLoader??"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2H_fWuTUVvFK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#export\n",
        "class DataBunch():\n",
        "    def __init__(self, train_dl, valid_dl, c=None):\n",
        "        self.train_dl,self.valid_dl,self.c = train_dl,valid_dl,c\n",
        "        \n",
        "    @property\n",
        "    def train_ds(self): return self.train_dl.dataset\n",
        "        \n",
        "    @property\n",
        "    def valid_ds(self): return self.valid_dl.dataset"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "abKX1ZSBYQbm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data = DataBunch(*get_dls(train_ds, valid_ds, bs), c)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LIM65IbkYR8N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#export\n",
        "def get_model(data, lr=0.5, nh=50):\n",
        "    m = data.train_ds.x.shape[1]\n",
        "    model = nn.Sequential(nn.Linear(m,nh), nn.ReLU(), nn.Linear(nh,data.c))\n",
        "    return model, optim.SGD(model.parameters(), lr=lr)\n",
        "\n",
        "class Learner():\n",
        "    def __init__(self, model, opt, loss_func, data):\n",
        "        self.model,self.opt,self.loss_func,self.data = model,opt,loss_func,data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jXn7pmhzYgeW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "learn = Learner(*get_model(data), loss_func, data)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NmiltzxncZgH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def fit(epochs, learn):\n",
        "    for epoch in range(epochs):\n",
        "        learn.model.train()\n",
        "        for xb,yb in learn.data.train_dl:\n",
        "            loss = learn.loss_func(learn.model(xb), yb)\n",
        "            loss.backward()\n",
        "            learn.opt.step()\n",
        "            learn.opt.zero_grad()\n",
        "\n",
        "        learn.model.eval()\n",
        "        with torch.no_grad():\n",
        "            tot_loss,tot_acc = 0.,0.\n",
        "            for xb,yb in learn.data.valid_dl:\n",
        "                pred = learn.model(xb)\n",
        "                tot_loss += learn.loss_func(pred, yb)\n",
        "                tot_acc  += accuracy (pred,yb)\n",
        "        nv = len(learn.data.valid_dl)\n",
        "        print(epoch, tot_loss/nv, tot_acc/nv)\n",
        "    return tot_loss/nv, tot_acc/nv"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PJwuEnF8ccwG",
        "colab_type": "code",
        "outputId": "903887cc-07c7-450a-f5aa-63f293185ab6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "loss,acc = fit(1, learn)"
      ],
      "execution_count": 123,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 tensor(0.1600) tensor(0.9536)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mXZhYkpgcjXb",
        "colab_type": "text"
      },
      "source": [
        "# CallbackHandler\n",
        "\n",
        "This was our training loop (without validation) from the previous notebook, with the inner loop contents factored out:\n",
        "\n",
        "\n",
        "```\n",
        "def one_batch(xb,yb):\n",
        "    pred = model(xb)\n",
        "    loss = loss_func(pred, yb)\n",
        "    loss.backward()\n",
        "    opt.step()\n",
        "    opt.zero_grad()\n",
        "\n",
        "def fit():\n",
        "    for epoch in range(epochs):\n",
        "        for b in train_dl: one_batch(*b)\n",
        "```\n",
        "\n",
        "Add callbacks so we can remove complexity from loop, and make it flexible:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R8CLNBXkcgQn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def one_batch(xb, yb, cb):\n",
        "    if not cb.begin_batch(xb,yb): return\n",
        "    loss = cb.learn.loss_func(cb.learn.model(xb), yb)\n",
        "    if not cb.after_loss(loss): return\n",
        "    loss.backward()\n",
        "    if cb.after_backward(): cb.learn.opt.step()\n",
        "    if cb.after_step(): cb.learn.opt.zero_grad()\n",
        "\n",
        "def all_batches(dl, cb):\n",
        "    for xb,yb in dl:\n",
        "        one_batch(xb, yb, cb)\n",
        "        if cb.do_stop(): return\n",
        "\n",
        "def fit(epochs, learn, cb):\n",
        "    if not cb.begin_fit(learn): return\n",
        "    for epoch in range(epochs):\n",
        "        if not cb.begin_epoch(epoch): continue\n",
        "        all_batches(learn.data.train_dl, cb)\n",
        "        \n",
        "        if cb.begin_validate():\n",
        "            with torch.no_grad(): all_batches(learn.data.valid_dl, cb)\n",
        "        if cb.do_stop() or not cb.after_epoch(): break\n",
        "    cb.after_fit()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e_G0coC2RqJi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Callback():\n",
        "    def begin_fit(self, learn):\n",
        "        self.learn = learn\n",
        "        return True\n",
        "    def after_fit(self): return True\n",
        "    def begin_epoch(self, epoch):\n",
        "        self.epoch=epoch\n",
        "        return True\n",
        "    def begin_validate(self): return True\n",
        "    def after_epoch(self): return True\n",
        "    def begin_batch(self, xb, yb):\n",
        "        self.xb,self.yb = xb,yb\n",
        "        return True\n",
        "    def after_loss(self, loss):\n",
        "        self.loss = loss\n",
        "        return True\n",
        "    def after_backward(self): return True\n",
        "    def after_step(self): return True"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rv0bXK8PRssc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class CallbackHandler():\n",
        "    def __init__(self,cbs=None):\n",
        "        self.cbs = cbs if cbs else []\n",
        "\n",
        "    def begin_fit(self, learn):\n",
        "        self.learn,self.in_train = learn,True\n",
        "        learn.stop = False\n",
        "        res = True\n",
        "        for cb in self.cbs: res = res and cb.begin_fit(learn)\n",
        "        return res\n",
        "\n",
        "    def after_fit(self):\n",
        "        res = not self.in_train\n",
        "        for cb in self.cbs: res = res and cb.after_fit()\n",
        "        return res\n",
        "    \n",
        "    def begin_epoch(self, epoch):\n",
        "        learn.model.train()\n",
        "        self.in_train=True\n",
        "        res = True\n",
        "        for cb in self.cbs: res = res and cb.begin_epoch(epoch)\n",
        "        return res\n",
        "\n",
        "    def begin_validate(self):\n",
        "        self.learn.model.eval()\n",
        "        self.in_train=False\n",
        "        res = True\n",
        "        for cb in self.cbs: res = res and cb.begin_validate()\n",
        "        return res\n",
        "\n",
        "    def after_epoch(self):\n",
        "        res = True\n",
        "        for cb in self.cbs: res = res and cb.after_epoch()\n",
        "        return res\n",
        "      \n",
        "    def begin_batch(self, xb, yb):\n",
        "        res = True\n",
        "        for cb in self.cbs: res = res and cb.begin_batch(xb, yb)\n",
        "        return res\n",
        "\n",
        "    def after_loss(self, loss):\n",
        "        res = self.in_train\n",
        "        for cb in self.cbs: res = res and cb.after_loss(loss)\n",
        "        return res\n",
        "\n",
        "    def after_backward(self):\n",
        "        res = True\n",
        "        for cb in self.cbs: res = res and cb.after_backward()\n",
        "        return res\n",
        "\n",
        "    def after_step(self):\n",
        "        res = True\n",
        "        for cb in self.cbs: res = res and cb.after_step()\n",
        "        return res\n",
        "    \n",
        "    def do_stop(self):\n",
        "        try:     return learn.stop\n",
        "        finally: learn.stop = False"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "px5OhzCiVqiz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class TestCallback(Callback):\n",
        "    def begin_fit(self,learn):\n",
        "        super().begin_fit(learn)\n",
        "        self.n_iters = 0\n",
        "        return True\n",
        "        \n",
        "    def after_step(self):\n",
        "        self.n_iters += 1\n",
        "        print(self.n_iters)\n",
        "        if self.n_iters>=10: learn.stop = True\n",
        "        return True"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M245fnahV7A0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 199
        },
        "outputId": "50f367d4-fc61-48bb-b854-b274e01f4c97"
      },
      "source": [
        "fit(1, learn, cb=CallbackHandler([TestCallback()]))"
      ],
      "execution_count": 128,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N5PVlCIgWJ7w",
        "colab_type": "text"
      },
      "source": [
        "This is roughly how fastai does it now (except that the handler can also change and return xb, yb, and loss). But let's see if we can make things simpler and more flexible, so that a single class has access to everything and can change anything at any time. The fact that we're passing cb to so many functions is a strong hint they should all be in the same class!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NVQt47poyTOH",
        "colab_type": "text"
      },
      "source": [
        "# Runner"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cuqmMMBkZ1mK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#export\n",
        "import re\n",
        "\n",
        "_camel_re1 = re.compile('(.)([A-Z][a-z]+)')\n",
        "_camel_re2 = re.compile('([a-z0-9])([A-Z])')\n",
        "\n",
        "def camel2snake(name):\n",
        "    s1 = re.sub(_camel_re1, r'\\1_\\2', name)\n",
        "    return re.sub(_camel_re2, r'\\1_\\2', s1).lower()\n",
        "  \n",
        "class Callback():\n",
        "    _order=0\n",
        "    def set_runner(self, run): self.run = run\n",
        "    def __getattr__(self, k): return getattr(self.run, k)\n",
        "    \n",
        "    @property \n",
        "    def name(self):\n",
        "      name = re.sub(r'Callback$','', self.__class__.__name__)\n",
        "      return camel2snake(name or 'callback')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "42WVi-y9Z4IL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class TrainEvalCallback(Callback):\n",
        "  def begin_fit(self):\n",
        "    self.run.n_epochs = 0.\n",
        "    self.run.n_iter = 0\n",
        "    \n",
        "  def after_batch(self):\n",
        "    if not self.in_train: return\n",
        "    self.run.n_epochs += 1./self.iters\n",
        "    self.run.n_iter += 1\n",
        "    \n",
        "  def begin_epoch(self):\n",
        "    self.run.n_epochs = self.epoch\n",
        "    self.model.train()\n",
        "    self.run.in_train = True\n",
        "    \n",
        "  def begin_validate(self):\n",
        "    self.model.eval()\n",
        "    self.run.in_train=False"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UY1F6HkV0CYp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class TestCallback(Callback):\n",
        "  def after_step(self):\n",
        "    if self.train_eval.n_iters>=10: return True"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i-5kWgFV301P",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "6057041f-4200-415d-fcae-2c0d5f7b4721"
      },
      "source": [
        "cbname = 'TrainEvalCallback'\n",
        "camel2snake(cbname)"
      ],
      "execution_count": 132,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'train_eval_callback'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 132
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3IKg2A5j32wa",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "8ca34c1d-208e-49fc-81e6-f495e8718241"
      },
      "source": [
        "TrainEvalCallback().name"
      ],
      "execution_count": 133,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'train_eval'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 133
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YQ7-ej5D39Vv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#export\n",
        "from typing import *\n",
        "\n",
        "def listify(o):\n",
        "    if o is None: return []\n",
        "    if isinstance(o, list): return o\n",
        "    if isinstance(o, Iterable): return list(o)\n",
        "    return [o]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zaAplbdB4hDB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Runner():\n",
        "  def __init__(self, cbs = None, cb_funcs = None):\n",
        "    cbs = listify(cbs)\n",
        "    for cbf in listify(cb_funcs):\n",
        "      cb = cbf()\n",
        "      setattr(self, cb.name, cb)\n",
        "      cbs.append(cb)\n",
        "    self.stop, self.cbs = False, [TrainEvalCallback()]+cbs\n",
        "    \n",
        "  @property\n",
        "  def opt(self):       return self.learn.opt\n",
        "  \n",
        "  @property\n",
        "  def model(self):     return self.learn.model\n",
        "  \n",
        "  @property\n",
        "  def loss_func(self): return self.learn.loss_func\n",
        "  \n",
        "  @property\n",
        "  def data(self):      return self.learn.data\n",
        "  \n",
        "  \n",
        "  def one_batch(self, xb, yb):\n",
        "    self.xb,self.yb = xb,yb\n",
        "    if self('begin_batch'): return \n",
        "    self.pred = self.model(self.xb)\n",
        "    if self('after_pred'): return\n",
        "    self.loss = self.loss_func(self.pred,self.yb)\n",
        "    if self('after_loss') or not self.in_train: return\n",
        "    self.loss.backward()\n",
        "    if self('after_backward'): return \n",
        "    self.opt.step()\n",
        "    if self('after_step'): return\n",
        "    self.opt.step()\n",
        "    if self('after_step'): return\n",
        "    self.opt.zero_grad()\n",
        "    \n",
        "    \n",
        "  def all_batches(self, dl):\n",
        "    self.iters = len(dl)\n",
        "    for xb, yb in dl:\n",
        "      if self.stop: break\n",
        "      self.one_batch(xb, yb)\n",
        "      self('after_batch')\n",
        "    self.stop = False\n",
        "    \n",
        "  \n",
        "  def fit(self, epochs, learn):\n",
        "    self.epochs, self.learn = epochs, learn\n",
        "    \n",
        "    try:\n",
        "      for cb in self.cbs: cb.set_runner(self)\n",
        "      if self('begin_fit'): return\n",
        "      for epoch in range(epochs):\n",
        "        self.epoch = epoch\n",
        "        if not self('begin_epoch'): self.all_batches(self.data.train_dl)\n",
        "          \n",
        "        with torch.no_grad():\n",
        "          if not self('begin_validate'): self.all_batches(self.data.valid_dl)\n",
        "            \n",
        "        if self('after_epoch'): break\n",
        "          \n",
        "    finally:\n",
        "      self('after_fit')\n",
        "      self.learn = None\n",
        "      \n",
        "      \n",
        "  def __call__(self, cb_name):\n",
        "    for cb in sorted(self.cbs, key = lambda x: x._order):\n",
        "      f = getattr(cb, cb_name, None)\n",
        "      if f and f(): return True\n",
        "    return False\n",
        "      \n",
        "   "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YbSyuBxWq89D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#export\n",
        "class AvgStats():\n",
        "  def __init__(self, metrics, in_train): self.metrics, self.in_train = listify(metrics), in_train\n",
        "    \n",
        "  def reset(self):\n",
        "    self.tot_loss, self.count = 0., 0\n",
        "    self.tot_mets = [0.]*len(self.metrics)\n",
        "    \n",
        "  @property\n",
        "  def all_stats(self): return [self.tot_loss.item()] + self.tot_mets\n",
        "  \n",
        "  @property\n",
        "  def avg_stats(self): return [o/self.count for o in self.all_stats]\n",
        "  \n",
        "  def __repr__(self):\n",
        "    if not self.count: return \"\"\n",
        "    return f\"{'train' if self.in_train else 'valid'}: {self.avg_stats}\"\n",
        "  \n",
        "  def accumulate(self, run):\n",
        "    bn = run.xb.shape[0]\n",
        "    self.tot_loss += run.loss*bn\n",
        "    self.count += bn\n",
        "    for i,m in enumerate(self.metrics):\n",
        "      self.tot_mets[i] += m(run.pred, run.yb)*bn\n",
        "      \n",
        "\n",
        "class AvgStatsCallback(Callback):\n",
        "    def __init__(self, metrics):\n",
        "        self.train_stats,self.valid_stats = AvgStats(metrics,True),AvgStats(metrics,False)\n",
        "        \n",
        "    def begin_epoch(self):\n",
        "        self.train_stats.reset()\n",
        "        self.valid_stats.reset()\n",
        "        \n",
        "    def after_loss(self):\n",
        "        stats = self.train_stats if self.in_train else self.valid_stats\n",
        "        with torch.no_grad(): stats.accumulate(self.run)\n",
        "    \n",
        "    def after_epoch(self):\n",
        "        print(self.train_stats)\n",
        "        print(self.valid_stats)\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "inFaRVOkyFtV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "learn = Learner(*get_model(data), loss_func, data)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qvi0vNWHyMPX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "stats = AvgStatsCallback([accuracy])\n",
        "run = Runner(cbs = stats)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1X1SBZpLyZO8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "0b18e023-84f2-49f9-d89c-97ffea7999f4"
      },
      "source": [
        "run.fit(1, learn)"
      ],
      "execution_count": 142,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train: [0.4106865625, tensor(0.8762)]\n",
            "valid: [0.283966796875, tensor(0.9169)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kBe8zPi-zGhw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}