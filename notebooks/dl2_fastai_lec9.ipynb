{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "dl2_fastai_lec9.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "dUKCK8DxNWHW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from pathlib import Path\n",
        "from IPython.core.debugger import set_trace\n",
        "from fastai import datasets\n",
        "import pickle, gzip, math, torch, matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "from torch import tensor\n",
        "import operator\n",
        "from torch.nn import init\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "MNIST_URL='http://deeplearning.net/data/mnist/mnist.pkl'\n",
        "\n",
        "\n",
        "def test(a,b,cmp,cname=None):\n",
        "    if cname is None: cname=cmp.__name__\n",
        "    assert cmp(a,b),f\"{cname}:\\n{a}\\n{b}\"\n",
        "\n",
        "def test_eq(a,b): test(a,b,operator.eq,'==')\n",
        "\n",
        "def near(a,b): return torch.allclose(a, b, rtol=1e-3, atol=1e-5)\n",
        "\n",
        "def test_near(a,b): test(a,b,near)  \n",
        "  \n",
        "def get_data():\n",
        "    path = datasets.download_data(MNIST_URL, ext='.gz')\n",
        "    with gzip.open(path, 'rb') as f:\n",
        "        ((x_train, y_train), (x_valid, y_valid), _) = pickle.load(f, encoding='latin-1')\n",
        "    return map(tensor, (x_train,y_train,x_valid,y_valid))\n",
        "\n",
        "def normalize(x, m, s): return (x-m)/s\n",
        "\n",
        "def test_near_zero(a,tol=1e-3): assert a.abs()<tol, f\"Near zero: {a}\"\n",
        "  \n",
        "def mse(output, targ): return (output.squeeze(-1) - targ).pow(2).mean()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hjZ3pyglN3wd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "\n",
        "%matplotlib inline"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cixl9tCxNu4s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "mpl.rcParams['image.cmap'] = 'gray'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BylmiT_bN0Vq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_train,y_train,x_valid,y_valid = get_data()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J8PmstIeXe3r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "n,m = x_train.shape\n",
        "c = y_train.max()+1\n",
        "nh = 50"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mlBXKvYuXjro",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Model(nn.Module):\n",
        "    def __init__(self, n_in, nh, n_out):\n",
        "        super().__init__()\n",
        "        self.layers = [nn.Linear(n_in,nh), nn.ReLU(), nn.Linear(nh,n_out)]\n",
        "        \n",
        "    def __call__(self, x):\n",
        "        for l in self.layers: x = l(x)\n",
        "        return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kfYdWsNhXoNm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = Model(m, nh, 10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GxuoB3PFXsUE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pred = model(x_train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5kugE0F2YEGF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "F??"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sgeZxLOGXt54",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def log_softmax(x): return (x.exp()/(x.exp().sum(-1,keepdim = True))).log() # why -1 and why keepdim?"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uKYjaO6zYyjw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test = torch.randn(5,10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Y75enF2Y3FC",
        "colab_type": "code",
        "outputId": "e60a6b27-0cfe-43c8-e3d5-30b7111e64e6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        }
      },
      "source": [
        "test,test.sum(0)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[ 0.3796,  0.8264, -0.8497, -0.0303,  1.0123,  0.2223,  1.0420,  1.0414,\n",
              "           1.6561, -0.1093],\n",
              "         [-0.3439,  0.3155,  0.0568, -0.6805, -0.6028, -0.8371, -0.5300,  0.9267,\n",
              "           0.5894, -1.1739],\n",
              "         [ 0.0818, -0.9784,  0.6176,  0.3624,  0.7847, -0.3085,  0.8961,  0.5564,\n",
              "           0.7844,  0.2580],\n",
              "         [ 1.3042, -0.4559,  1.0494,  0.2993,  1.7560,  0.6555,  1.9412, -1.2190,\n",
              "          -0.0827,  0.9786],\n",
              "         [-0.6750, -0.9882, -0.1480, -0.5307, -0.5577, -0.5488, -0.2529,  0.2207,\n",
              "          -0.5769, -2.3232]]),\n",
              " tensor([ 0.7466, -1.2805,  0.7261, -0.5798,  2.3925, -0.8165,  3.0964,  1.5262,\n",
              "          2.3702, -2.3697]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AobgzTLdY-Qs",
        "colab_type": "code",
        "outputId": "534f51b7-e95c-4b06-e136-7aaeec3dc709",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "test.sum(0,keepdim = True)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 0.7466, -1.2805,  0.7261, -0.5798,  2.3925, -0.8165,  3.0964,  1.5262,\n",
              "          2.3702, -2.3697]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lfly2gXIZRpL",
        "colab_type": "code",
        "outputId": "11f4be6e-7dcf-40fc-be18-e1f4b0eb636a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 108
        }
      },
      "source": [
        "test.sum(-1),test.sum(-1,keepdim = True)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([ 5.1909, -2.2798,  3.0545,  6.2265, -6.3806]), tensor([[ 5.1909],\n",
              "         [-2.2798],\n",
              "         [ 3.0545],\n",
              "         [ 6.2265],\n",
              "         [-6.3806]]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K-1AN6OkZYBB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sm_pred = log_softmax(pred)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o6fvB1ufZv_v",
        "colab_type": "text"
      },
      "source": [
        "The cross entropy loss for some target $x$ and some prediction $p(x)$ is given by:\n",
        "\n",
        "$$ -\\sum x\\, \\log p(x) $$\n",
        "But since our $x$s are 1-hot encoded, this can be rewritten as $-\\log(p_{i})$ where i is the index of the desired target.\n",
        "\n",
        "This can be done using numpy-style integer array indexing. Note that PyTorch supports all the tricks in the advanced indexing methods discussed in that link."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qWJaMXa_ZvOn",
        "colab_type": "code",
        "outputId": "7a676c3d-1c61-40c2-d955-ea550c26deb4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "y_train[:3]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([5, 0, 4])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IqvEfPLmsblP",
        "colab_type": "code",
        "outputId": "3518860f-daac-4260-a5f4-d4bd09f5985e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "sm_pred.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([50000, 10])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c9TE4nlBwEDR",
        "colab_type": "text"
      },
      "source": [
        "Indexing ways"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rTCCbuIBvkZy",
        "colab_type": "code",
        "outputId": "fdf89dd5-b2e9-4141-9e08-a49eb857f10f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 126
        }
      },
      "source": [
        "sm_pred[[1,2,3]]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-2.2367, -2.2286, -2.1919, -2.6077, -2.2727, -2.1632, -2.5577, -2.1643,\n",
              "         -2.2809, -2.4349],\n",
              "        [-2.2286, -2.3475, -2.2372, -2.5221, -2.2497, -2.2005, -2.3702, -2.1699,\n",
              "         -2.3387, -2.4147],\n",
              "        [-2.1963, -2.2952, -2.3481, -2.5119, -2.2377, -2.1865, -2.4415, -2.1769,\n",
              "         -2.2887, -2.4019]], grad_fn=<IndexBackward>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4sN11oTRxPyT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LAkqKTqTwBZR",
        "colab_type": "code",
        "outputId": "376164c3-e151-4e8e-c819-a143a1044a14",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "sm_pred[[0,1,2],[5, 0, 4]]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([-2.2266, -2.2367, -2.2497], grad_fn=<IndexBackward>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RwRBkuNzwOp6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def nll(input, target): return -input[range(target.shape[0]),target].mean()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rpCLAT_JUtOS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "loss = nll(sm_pred, y_train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HdgJnTOAxFQl",
        "colab_type": "code",
        "outputId": "dfbc8cc8-960e-4065-9bde-8ca7ea660637",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "loss"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(2.3195, grad_fn=<NegBackward>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BPa0OkoVyBhZ",
        "colab_type": "text"
      },
      "source": [
        "Note that the formula\n",
        "\n",
        "$$\\log \\left ( \\frac{a}{b} \\right ) = \\log(a) - \\log(b)$$\n",
        "gives a simplification when we compute the log softmax, which was previously defined as (x.exp()/(x.exp().sum(-1,keepdim=True))).log()"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "737QMBWCx8qI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def log_softmax(x): return x - x.exp().sum(-1,keepdim=True).log()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DR5DjTD1yIWt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_near(nll(log_softmax(pred), y_train), loss)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G4lUz7SayuVe",
        "colab_type": "text"
      },
      "source": [
        "Then, there is a way to compute the log of the sum of exponentials in a more stable way, called the LogSumExp trick. The idea is to use the following formula:\n",
        "\n",
        "$$\\log \\left ( \\sum_{j=1}^{n} e^{x_{j}} \\right ) = \\log \\left ( e^{a} \\sum_{j=1}^{n} e^{x_{j}-a} \\right ) = a + \\log \\left ( \\sum_{j=1}^{n} e^{x_{j}-a} \\right )$$\n",
        "where a is the maximum of the $x_{j}$."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NFWWFnrsyLZx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def logsumexp(x):\n",
        "    m = x.max(-1)[0]\n",
        "    return m + (x-m[:,None]).exp().sum(-1).log()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k7OwMWv82iDe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_near(logsumexp(pred), pred.logsumexp(-1))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NAL6uLft2nmm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def log_softmax(x): return x - x.logsumexp(-1,keepdim=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vr7VNvjG2t3Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_near(nll(log_softmax(pred), y_train), loss)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-4SlrK3c2xcI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#PyTorch's implementation.\n",
        "test_near(F.nll_loss(F.log_softmax(pred, -1), y_train), loss)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WM3utHEI22Cm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#In PyTorch, F.log_softmax and F.nll_loss are combined in one optimized function, F.cross_entropy\n",
        "test_near(F.cross_entropy(pred, y_train), loss)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "piDX1oHq3Eqk",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "Basic training loop\n",
        "Basically the training loop repeats over the following steps:\n",
        "\n",
        "1.get the output of the model on a batch of inputs\n",
        "\n",
        "2.compare the output to the labels we have and compute a loss\n",
        "\n",
        "3.calculate the gradients of the loss with respect to every parameter of the model\n",
        "\n",
        "4.update said parameters with those gradients to make them a little bit better"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lb0jpVha3CJB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "loss_func = F.cross_entropy"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8cOoS8lu3zzb",
        "colab_type": "code",
        "outputId": "5de6a7f8-1f12-465f-d2d6-22c6a1ecc99e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "source": [
        "temp = tensor([[0,1,0],[1,0,0],[1,0,0]]).float()\n",
        "temp"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0., 1., 0.],\n",
              "        [1., 0., 0.],\n",
              "        [1., 0., 0.]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i1BFUSnJ45kv",
        "colab_type": "code",
        "outputId": "5d3f1999-073e-4afd-dd37-f5ad1bb3d8c1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "torch.argmax(temp,dim=1),torch.argmax(temp,dim=0)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([1, 0, 0]), tensor([2, 0, 2]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZMtbYr1o3X5S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def accuracy(out, yb): return (torch.argmax(out,dim=1)==yb).float().mean()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "my07FKMQ5C1-",
        "colab_type": "code",
        "outputId": "c4c65d8c-f4f4-442f-9f4d-d95decec78e7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "bs = 64\n",
        "\n",
        "xb = x_train[:bs]\n",
        "preds = model(xb)\n",
        "preds[0], preds.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([ 0.1614,  0.0266,  0.1104, -0.1878, -0.0250,  0.1070, -0.1843,  0.1994,\n",
              "          0.0936, -0.0735], grad_fn=<SelectBackward>), torch.Size([64, 10]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NpAup36Q5lM0",
        "colab_type": "code",
        "outputId": "8fc594d6-eb79-4db4-9581-08f9eab4e541",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "yb = y_train[0:bs]\n",
        "loss_func(preds, yb)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(2.3244, grad_fn=<NllLossBackward>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oMhIEx5B7QHw",
        "colab_type": "code",
        "outputId": "d758dbd1-a420-4d78-b901-eb55a014a629",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "accuracy(preds, yb)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(0.1094)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-4hZ9z0H7UcS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lr = 0.1 \n",
        "epochs = 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KAJURLbf7v4m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "n = x_train.shape[0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dHh6wWVOKHsJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "nn.Linear??"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R48uyBdh7htX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for epoch in range(epochs):\n",
        "  for batch in range((n-1)//bs +1):\n",
        "    \n",
        "    x_batch = x_train[batch*bs:(batch+1)*bs]\n",
        "    y_batch = y_train[batch*bs:(batch+1)*bs]\n",
        "    \n",
        "    preds = model(x_batch)\n",
        "    \n",
        "    loss = loss_func(preds,y_batch)\n",
        "    \n",
        "    loss.backward()\n",
        "    \n",
        "    with torch.no_grad():\n",
        "      for l in model.layers:\n",
        "        if hasattr(l, 'weight'):\n",
        "          l.weight -= l.weight.grad*lr\n",
        "          l.bias -= l.bias.grad*lr\n",
        "          l.weight.grad.zero_()\n",
        "          l.bias.grad.zero_()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "umDCBQQZE-mC",
        "colab_type": "code",
        "outputId": "e60083d3-362f-4cba-c972-0ec3aba545aa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "loss_func(model(xb), yb), accuracy(model(xb), yb)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor(0.2988, grad_fn=<NllLossBackward>), tensor(0.9219))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "duhixw1hUhFv",
        "colab_type": "text"
      },
      "source": [
        "Using parameters and optim\n",
        "\n",
        "Parameters\n",
        "\n",
        "Use nn.Module.__setattr__ and move relu to functional"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "og9F3S4QFKgF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Model(nn.Module):\n",
        "    def __init__(self, n_in, nh, n_out):\n",
        "        super().__init__()\n",
        "        self.l1 = nn.Linear(n_in,nh)\n",
        "        self.l2 = nn.Linear(nh,n_out)\n",
        "        \n",
        "    def __call__(self, x): return self.l2(F.relu(self.l1(x)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gyloIqjATJOV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = Model(m, nh, 10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jl97hFDyUxVA",
        "colab_type": "code",
        "outputId": "33eba30d-eaea-4a8b-d492-ae48a5bf56d7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "for name,l in model.named_children(): print(f\"{name}: {l}\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "l1: Linear(in_features=784, out_features=50, bias=True)\n",
            "l2: Linear(in_features=50, out_features=10, bias=True)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KIMtNCl6U5fH",
        "colab_type": "code",
        "outputId": "0c8f3dac-ff55-4264-d09f-49edeb729484",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        }
      },
      "source": [
        "model"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Model(\n",
              "  (l1): Linear(in_features=784, out_features=50, bias=True)\n",
              "  (l2): Linear(in_features=50, out_features=10, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LcKKrtbWVKJ5",
        "colab_type": "code",
        "outputId": "73459e5c-a206-4ffd-ec4f-8dd13831453f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "model.l1"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Linear(in_features=784, out_features=50, bias=True)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "naiMuzMWVO3A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def fit():\n",
        "    for epoch in range(epochs):\n",
        "        for i in range((n-1)//bs + 1):\n",
        "            start_i = i*bs\n",
        "            end_i = start_i+bs\n",
        "            xb = x_train[start_i:end_i]\n",
        "            yb = y_train[start_i:end_i]\n",
        "            loss = loss_func(model(xb), yb)\n",
        "\n",
        "            loss.backward()\n",
        "            with torch.no_grad():\n",
        "                for p in model.parameters(): p -= p.grad * lr\n",
        "                model.zero_grad()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GD6TOHC6aHmQ",
        "colab_type": "code",
        "outputId": "5e065f64-18ef-44ae-aec7-d6737c447584",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "fit()\n",
        "loss_func(model(xb), yb), accuracy(model(xb), yb)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor(0.2986, grad_fn=<NllLossBackward>), tensor(0.9219))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WQ7WFG6laWRb",
        "colab_type": "text"
      },
      "source": [
        "Behind the scenes, PyTorch overrides the __setattr__ function in nn.Module so that the submodules you define are properly registered as parameters of the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "84eenkEZaLSB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class DummyModule():\n",
        "    def __init__(self, n_in, nh, n_out):\n",
        "        self._modules = {}\n",
        "        self.l1 = nn.Linear(n_in,nh)\n",
        "        self.l2 = nn.Linear(nh,n_out)\n",
        "        self.n = 3\n",
        "        \n",
        "    def __setattr__(self,k,v):\n",
        "        if not k.startswith(\"_\") and hasattr(v,'parameters'): self._modules[k] = v\n",
        "        super().__setattr__(k,v)\n",
        "        \n",
        "    def __repr__(self): return f'{self._modules}'\n",
        "    \n",
        "    def parameters(self):\n",
        "        for l in self._modules.values():\n",
        "            for p in l.parameters(): yield p"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9mgsTewnaerl",
        "colab_type": "code",
        "outputId": "e1cc22db-900d-4bc6-e668-31bd0f15104a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        }
      },
      "source": [
        "mdl = DummyModule(m,nh,10)\n",
        "mdl"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'l1': Linear(in_features=784, out_features=50, bias=True), 'l2': Linear(in_features=50, out_features=10, bias=True)}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KlhXNJQCa4ZP",
        "colab_type": "code",
        "outputId": "f24e7c0c-92e6-4a68-dced-ac7c00cf8878",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        }
      },
      "source": [
        "[o.shape for o in mdl.parameters()]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[torch.Size([50, 784]),\n",
              " torch.Size([50]),\n",
              " torch.Size([10, 50]),\n",
              " torch.Size([10])]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ANDcit_NehxT",
        "colab_type": "text"
      },
      "source": [
        "**Registering modules**\n",
        "\n",
        "We can use the original layers approach, but we have to register the modules"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W0RSQvLFbAWA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "layers = [nn.Linear(m,nh), nn.ReLU(), nn.Linear(nh,10)]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cm_eV-wPcDHG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Model(nn.Module):\n",
        "    def __init__(self, layers):\n",
        "        super().__init__()\n",
        "        self.layers = layers\n",
        "        for i,l in enumerate(self.layers): self.add_module(f'layer_{i}', l)\n",
        "        \n",
        "    def __call__(self, x):\n",
        "        for l in self.layers: x = l(x)\n",
        "        return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xS0ii6xJev8W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = Model(layers)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VQ6UoWC1e0E2",
        "colab_type": "code",
        "outputId": "2963a652-2266-4d4a-c1b3-2363d87787e9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 108
        }
      },
      "source": [
        "model"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Model(\n",
              "  (layer_0): Linear(in_features=784, out_features=50, bias=True)\n",
              "  (layer_1): ReLU()\n",
              "  (layer_2): Linear(in_features=50, out_features=10, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V8X3CbG7fIF-",
        "colab_type": "text"
      },
      "source": [
        "**nn.ModuleList**\n",
        "\n",
        "nn.ModuleList does this for us"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X6t3-sVPe1tr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class SequentialModel(nn.Module):\n",
        "    def __init__(self, layers):\n",
        "        super().__init__()\n",
        "        self.layers = nn.ModuleList(layers)\n",
        "        \n",
        "    def __call__(self, x):\n",
        "        for l in self.layers: x = l(x)\n",
        "        return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tHuXCPpifPdH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = SequentialModel(layers)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yr6rWTfDfRP3",
        "colab_type": "code",
        "outputId": "4eb5cb1f-087b-4345-b503-2e0e3c8212ed",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 145
        }
      },
      "source": [
        "model"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "SequentialModel(\n",
              "  (layers): ModuleList(\n",
              "    (0): Linear(in_features=784, out_features=50, bias=True)\n",
              "    (1): ReLU()\n",
              "    (2): Linear(in_features=50, out_features=10, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zqPPz2mhfS1B",
        "colab_type": "code",
        "outputId": "d4e36e75-efeb-4219-b348-36df5a9912f0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "fit()\n",
        "loss_func(model(xb), yb), accuracy(model(xb), yb)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor(0.2909, grad_fn=<NllLossBackward>), tensor(0.9219))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5v8HuspGflrO",
        "colab_type": "text"
      },
      "source": [
        "**nn.Sequential**\n",
        "\n",
        "nn.Sequential is a convenient class which does the same as the above:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mPUdXgKSfVXi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = nn.Sequential(nn.Linear(m,nh), nn.ReLU(), nn.Linear(nh,10))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bpUMMEbhiOdS",
        "colab_type": "code",
        "outputId": "4589f65f-92f3-4b53-cc2e-e9db5a28184e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "fit()\n",
        "loss_func(model(xb), yb), accuracy(model(xb), yb)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor(0.3058, grad_fn=<NllLossBackward>), tensor(0.9219))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "md3eV2aLiQbz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "nn.Sequential??"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "va-5gxUziS1W",
        "colab_type": "code",
        "outputId": "0de779df-c596-4551-c5ba-a4792a53cd4d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 108
        }
      },
      "source": [
        "model"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Sequential(\n",
              "  (0): Linear(in_features=784, out_features=50, bias=True)\n",
              "  (1): ReLU()\n",
              "  (2): Linear(in_features=50, out_features=10, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6CYQSFwtilCU",
        "colab_type": "text"
      },
      "source": [
        "**optim**\n",
        "\n",
        "Let's replace our previous manually coded optimization step\n",
        ":\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "with torch.no_grad():\n",
        "    for p in model.parameters(): p -= p.grad * lr\n",
        "    model.zero_grad()\n",
        "```\n",
        "\n",
        "\n",
        "    \n",
        "and instead use just:\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "opt.step()\n",
        "opt.zero_grad()\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iyZR68_MijxD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Optimizer():\n",
        "  \n",
        "  def __init__(self, params, lr=0.5): self.params,self.lr=list(params),lr\n",
        "    \n",
        "  def step(self):\n",
        "    with torch.no_grad():\n",
        "      for p in self.params: p -= p.grad*lr\n",
        "        \n",
        "  def zero_grad(self):\n",
        "    for p in self.params: p.grad.data.zero_()\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KL6OeNsgjjKz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = nn.Sequential(nn.Linear(m,nh), nn.ReLU(), nn.Linear(nh,10))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zGivvEhHjmCo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "opt = Optimizer(model.parameters())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DkrOfewsjoIN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for epoch in range(epochs):\n",
        "    for i in range((n-1)//bs + 1):\n",
        "        start_i = i*bs\n",
        "        end_i = start_i+bs\n",
        "        xb = x_train[start_i:end_i]\n",
        "        yb = y_train[start_i:end_i]\n",
        "        pred = model(xb)\n",
        "        loss = loss_func(pred, yb)\n",
        "\n",
        "        loss.backward()\n",
        "        opt.step()\n",
        "        opt.zero_grad()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ir4O1DSilD7_",
        "colab_type": "code",
        "outputId": "73cc50ea-21f6-482b-9000-cb3062e24108",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "loss,acc = loss_func(model(xb), yb), accuracy(model(xb), yb)\n",
        "loss,acc"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor(0.1438, grad_fn=<NllLossBackward>), tensor(0.9375))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 73
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "andZAjaHlM72",
        "colab_type": "text"
      },
      "source": [
        "PyTorch already provides this exact functionality in optim.SGD (it also handles stuff like momentum, which we'll look at later - except we'll be doing it in a more flexible way!)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mmH_-L2JlGv8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#export\n",
        "from torch import optim"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WiIAbOh5lV0z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "optim.SGD.step??"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CziWLb_XlYlq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_model():\n",
        "    model = nn.Sequential(nn.Linear(m,nh), nn.ReLU(), nn.Linear(nh,10))\n",
        "    return model, optim.SGD(model.parameters(), lr=lr)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KB1cYFDKrSr2",
        "colab_type": "code",
        "outputId": "11aff756-3c32-4357-aa9f-43d37c89bcd7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "model,opt = get_model()\n",
        "loss_func(model(xb), yb)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(2.3230, grad_fn=<NllLossBackward>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 77
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aFYdlUxFrVdg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for epoch in range(epochs):\n",
        "    for i in range((n-1)//bs + 1):\n",
        "        start_i = i*bs\n",
        "        end_i = start_i+bs\n",
        "        xb = x_train[start_i:end_i]\n",
        "        yb = y_train[start_i:end_i]\n",
        "        pred = model(xb)\n",
        "        loss = loss_func(pred, yb)\n",
        "\n",
        "        loss.backward()\n",
        "        opt.step()\n",
        "        opt.zero_grad()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iskbinMercoi",
        "colab_type": "code",
        "outputId": "171ade62-90aa-4cc6-c5e3-bc61614352d6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "loss,acc = loss_func(model(xb), yb), accuracy(model(xb), yb)\n",
        "loss,acc"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor(0.1529, grad_fn=<NllLossBackward>), tensor(1.))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 79
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F715hr6frfnO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "assert acc>0.7"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X-9d-jZarj-8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#export\n",
        "class Dataset():\n",
        "    def __init__(self, x, y): self.x,self.y = x,y\n",
        "    def __len__(self): return len(self.x)\n",
        "    def __getitem__(self,i): return self.x[i],self.y[i]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GyB_uDJPr4uo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_ds,valid_ds = Dataset(x_train,y_train), Dataset(x_valid,y_valid)\n",
        "assert len(train_ds)==len(x_train)\n",
        "assert len(valid_ds)==len(x_valid)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QHEUtkF7ssIx",
        "colab_type": "code",
        "outputId": "4ba6fc06-14fc-4c88-f180-465ca20a0ef9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 108
        }
      },
      "source": [
        "xb,yb = train_ds[0:5]\n",
        "assert xb.shape==(5,28*28)\n",
        "assert yb.shape==(5,)\n",
        "xb,yb"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
              "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "         [0., 0., 0.,  ..., 0., 0., 0.]]), tensor([5, 0, 4, 1, 9]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 83
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cGTjmeMSs8CE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model,opt = get_model()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H1Y3QdeEs_wy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for epoch in range(epochs):\n",
        "    for i in range((n-1)//bs + 1):\n",
        "        xb,yb = train_ds[i*bs : i*bs+bs]\n",
        "        pred = model(xb)\n",
        "        loss = loss_func(pred, yb)\n",
        "\n",
        "        loss.backward()\n",
        "        opt.step()\n",
        "        opt.zero_grad()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9B52JG0ktUD7",
        "colab_type": "code",
        "outputId": "53d0ca17-ca0a-42ba-dd33-b44aab626cd1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "loss,acc = loss_func(model(xb), yb), accuracy(model(xb), yb)\n",
        "assert acc>0.7\n",
        "loss,acc"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor(0.1424, grad_fn=<NllLossBackward>), tensor(1.))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 86
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cKhA_HrmucNv",
        "colab_type": "text"
      },
      "source": [
        "**DataLoader**\n",
        "\n",
        "Previously, our loop iterated over batches (xb, yb) like this:\n",
        "\n",
        "\n",
        "```\n",
        "for i in range((n-1)//bs + 1):\n",
        "    xb,yb = train_ds[i*bs : i*bs+bs]\n",
        "    ...\n",
        "```\n",
        "Let's make our loop much cleaner, using a data loader:\n",
        "\n",
        "\n",
        "```\n",
        "for xb,yb in train_dl:\n",
        "```\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jdYmexnutZmI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class DataLoader():\n",
        "  def __init__(self, ds, bs): self.ds, self.bs = ds,bs\n",
        "  def __iter__(self):\n",
        "    for i in range(0, len(self.ds), self.bs): yield self.ds[i:i+self.bs] # note here yield is used"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tBgkn9YfvzD8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_dl = DataLoader(train_ds, bs)\n",
        "valid_dl = DataLoader(valid_ds, bs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_TRdEiM4v-pz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "xb,yb = next(iter(valid_dl))\n",
        "assert xb.shape==(bs,28*28)\n",
        "assert yb.shape==(bs,)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UY47_Y_hwA-4",
        "colab_type": "code",
        "outputId": "729a7521-19de-4c8c-eec6-b2a4a44f4614",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 287
        }
      },
      "source": [
        "plt.imshow(xb[0].view(28,28))\n",
        "yb[0]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(3)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 90
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nXGStUmRfAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zWxGsrprwMH_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model,opt = get_model()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q1wXWEfpwqPK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def fit():\n",
        "    for epoch in range(epochs):\n",
        "        for xb,yb in train_dl:\n",
        "            pred = model(xb)\n",
        "            loss = loss_func(pred, yb)\n",
        "            loss.backward()\n",
        "            opt.step()\n",
        "            opt.zero_grad()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JOwm0y4Gwr_w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "fit()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FjH_wVquws8A",
        "colab_type": "code",
        "outputId": "06d3da72-d2f2-41c0-8e4c-3211287c9a56",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "loss,acc = loss_func(model(xb), yb), accuracy(model(xb), yb)\n",
        "assert acc>0.7\n",
        "loss,acc"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor(0.2725, grad_fn=<NllLossBackward>), tensor(0.9531))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 94
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "31zJUfXZyi5D",
        "colab_type": "text"
      },
      "source": [
        "**Random sampling**\n",
        "\n",
        "We want our training set to be in a random order, and that order should differ each iteration. But the validation set shouldn't be randomized."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zP5fyO3SwwzY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Sampler():\n",
        "    def __init__(self, ds, bs, shuffle=False):\n",
        "        self.n,self.bs,self.shuffle = len(ds),bs,shuffle\n",
        "        \n",
        "    def __iter__(self):\n",
        "        self.idxs = torch.randperm(self.n) if self.shuffle else torch.arange(self.n)\n",
        "        for i in range(0, self.n, self.bs): yield self.idxs[i:i+self.bs]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QfNdGn3CxyOJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "small_ds = Dataset(*train_ds[:10])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "43n3gwrixz-W",
        "colab_type": "code",
        "outputId": "635dc482-38f3-464e-f8af-90f84263aed1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "s = Sampler(small_ds,3,False)\n",
        "[o for o in s]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[tensor([0, 1, 2]), tensor([3, 4, 5]), tensor([6, 7, 8]), tensor([9])]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 97
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Fv-VikxyIxo",
        "colab_type": "code",
        "outputId": "61e0684a-99ed-4949-8b4f-93bbc4c01cc2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "s = Sampler(small_ds,3,True)\n",
        "[o for o in s]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[tensor([9, 2, 3]), tensor([1, 5, 8]), tensor([6, 0, 4]), tensor([7])]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 98
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_nd-5nLDyO2t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def collate(b):\n",
        "    xs,ys = zip(*b)\n",
        "    return torch.stack(xs),torch.stack(ys)\n",
        "  \n",
        "class DataLoader():\n",
        "    def __init__(self, ds, sampler, collate_fn=collate):\n",
        "        self.ds,self.sampler,self.collate_fn = ds,sampler,collate_fn\n",
        "        \n",
        "    def __iter__(self):\n",
        "        for s in self.sampler: yield self.collate_fn([self.ds[i] for i in s])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PvvU2YpHyW3U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_samp = Sampler(train_ds, bs, shuffle=True)\n",
        "valid_samp = Sampler(valid_ds, bs, shuffle=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OMhmI-1WygbZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_dl = DataLoader(train_ds, sampler=train_samp, collate_fn=collate)\n",
        "valid_dl = DataLoader(valid_ds, sampler=valid_samp, collate_fn=collate)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NzLiaH4cy22a",
        "colab_type": "code",
        "outputId": "df0357d2-4021-4621-d5ea-9ba1379022f0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 287
        }
      },
      "source": [
        "xb,yb = next(iter(valid_dl))\n",
        "plt.imshow(xb[0].view(28,28))\n",
        "yb[0]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(3)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 102
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nXGStUmRfAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5EnONtBI3kvX",
        "colab_type": "code",
        "outputId": "77a321ab-64da-476b-8520-d3b618aecbe4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 287
        }
      },
      "source": [
        "xb,yb = next(iter(train_dl))\n",
        "plt.imshow(xb[0].view(28,28))\n",
        "yb[0]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(2)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 103
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nv7KaaTZi/atrAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-V7654P23nm9",
        "colab_type": "code",
        "outputId": "42d91aad-63c6-4b82-8440-f5b23dce48c4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 287
        }
      },
      "source": [
        "xb,yb = next(iter(train_dl))\n",
        "plt.imshow(xb[0].view(28,28))\n",
        "yb[0]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(6)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 104
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4aBdLj6z3szx",
        "colab_type": "code",
        "outputId": "2d9ea43e-b876-4e73-a738-735fe1ff3dcf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "model,opt = get_model()\n",
        "fit()\n",
        "\n",
        "loss,acc = loss_func(model(xb), yb), accuracy(model(xb), yb)\n",
        "assert acc>0.7\n",
        "loss,acc"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor(0.3714, grad_fn=<NllLossBackward>), tensor(0.9375))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 105
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wQgDoLiQ4Nui",
        "colab_type": "text"
      },
      "source": [
        "**PyTorch DataLoader**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0l_Js-wW3xRo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torch.utils.data import DataLoader, SequentialSampler, RandomSampler"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zMwt9YbJ4Tnh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_dl = DataLoader(train_ds, bs, sampler=RandomSampler(train_ds), collate_fn=collate)\n",
        "valid_dl = DataLoader(valid_ds, bs, sampler=SequentialSampler(valid_ds), collate_fn=collate)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WLGaqYjH4X72",
        "colab_type": "code",
        "outputId": "98e6a8fa-f6e5-429c-e12b-2309157b2de8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "model,opt = get_model()\n",
        "fit()\n",
        "loss_func(model(xb), yb), accuracy(model(xb), yb)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor(0.3445, grad_fn=<NllLossBackward>), tensor(0.9219))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 108
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fVmBiX7265SN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "DataLoader??"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qMJPWXTo4aGL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_dl = DataLoader(train_ds, bs, shuffle=True, drop_last=True)\n",
        "valid_dl = DataLoader(valid_ds, bs, shuffle=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R6JYlHP_7cSH",
        "colab_type": "text"
      },
      "source": [
        "PyTorch's defaults work fine for most things however:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gXs-_RrB4d3v",
        "colab_type": "code",
        "outputId": "e1a04126-713a-4151-93cd-03e12d877318",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "model,opt = get_model()\n",
        "fit()\n",
        "\n",
        "loss,acc = loss_func(model(xb), yb), accuracy(model(xb), yb)\n",
        "assert acc>0.7\n",
        "loss,acc"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor(0.3673, grad_fn=<NllLossBackward>), tensor(0.9062))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 111
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0masRS8S7kRd",
        "colab_type": "text"
      },
      "source": [
        "Note that PyTorch's DataLoader, if you pass num_workers, will use multiple threads to call your Dataset.\n",
        "\n",
        "\n",
        "\n",
        "**Validation**\n",
        "\n",
        "You always should also have a validation set, in order to identify if you are overfitting.\n",
        "\n",
        "We will calculate and print the validation loss at the end of each epoch.\n",
        "\n",
        "(Note that we always call model.train() before training, and model.eval() before inference, because these are used by layers such as nn.BatchNorm2d and nn.Dropout to ensure appropriate behaviour for these different phases.)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2NY_fgiD7Gpo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def fit(epochs, model, loss_func, opt, train_dl, valid_dl):\n",
        "    for epoch in range(epochs):\n",
        "        # Handle batchnorm / dropout\n",
        "        model.train()\n",
        "#         print(model.training)\n",
        "        for xb,yb in train_dl:\n",
        "            loss = loss_func(model(xb), yb)\n",
        "            loss.backward()\n",
        "            opt.step()\n",
        "            opt.zero_grad()\n",
        "        \n",
        "        model.eval()\n",
        "        \n",
        "        with torch.no_grad():\n",
        "          # we calculate loss and accuracy on valid set which is displayed as metric after each epoch\n",
        "          tot_loss, tot_acc = 0.,0.\n",
        "          for xb, yb in valid_dl:  \n",
        "            pred = model(xb)\n",
        "            tot_loss += loss_func(pred, yb)\n",
        "            tot_acc += accuracy(pred,yb)\n",
        "        nv = len(valid_dl)\n",
        "        print(epoch, tot_loss/nv, tot_acc/nv)\n",
        "        \n",
        "    return tot_loss/nv, tot_acc/nv\n",
        "         "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jqcZwMYo-Yep",
        "colab_type": "text"
      },
      "source": [
        "Question: Are these validation results correct if batch size varies?\n",
        "\n",
        "get_dls returns dataloaders for the training and validation sets:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bzjJu0sv8A_o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#export\n",
        "def get_dls(train_ds, valid_ds, bs, **kwargs):\n",
        "    return (DataLoader(train_ds, batch_size=bs, shuffle=True, **kwargs),\n",
        "            DataLoader(valid_ds, batch_size=bs*2, **kwargs))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j8_KHoGn8EN1",
        "colab_type": "code",
        "outputId": "8b4000fe-72a1-49bf-995f-d9f70105404d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 108
        }
      },
      "source": [
        "train_dl,valid_dl = get_dls(train_ds, valid_ds, bs)\n",
        "model,opt = get_model()\n",
        "loss,acc = fit(5, model, loss_func, opt, train_dl, valid_dl)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 tensor(0.2954) tensor(0.9129)\n",
            "1 tensor(0.2248) tensor(0.9362)\n",
            "2 tensor(0.1870) tensor(0.9469)\n",
            "3 tensor(0.1571) tensor(0.9585)\n",
            "4 tensor(0.1383) tensor(0.9634)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O1nRVOXU-vn_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "assert acc>0.9"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iRwdYrYlViq5",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "# DataBunch/Learner"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B8WTIKYz-5oX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_train,y_train,x_valid,y_valid = get_data()\n",
        "train_ds,valid_ds = Dataset(x_train, y_train),Dataset(x_valid, y_valid)\n",
        "nh,bs = 50,64\n",
        "c = y_train.max().item()+1\n",
        "loss_func = F.cross_entropy"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p1F5tRczVwZX",
        "colab_type": "text"
      },
      "source": [
        "Factor out the connected pieces of info out of the fit() argument list\n",
        "\n",
        "fit(epochs, model, loss_func, opt, train_dl, valid_dl)\n",
        "\n",
        "Let's replace it with something that looks like this:\n",
        "\n",
        "fit(1, learn)\n",
        "\n",
        "This will allow us to tweak what's happening inside the training loop in other places of the code because the Learner object will be mutable, so changing any of its attribute elsewhere will be seen in our training loop"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x4R5hLsmWspv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "DataLoader??"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2H_fWuTUVvFK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#export\n",
        "class DataBunch():\n",
        "    def __init__(self, train_dl, valid_dl, c=None):\n",
        "        self.train_dl,self.valid_dl,self.c = train_dl,valid_dl,c\n",
        "        \n",
        "    @property\n",
        "    def train_ds(self): return self.train_dl.dataset\n",
        "        \n",
        "    @property\n",
        "    def valid_ds(self): return self.valid_dl.dataset"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "abKX1ZSBYQbm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data = DataBunch(*get_dls(train_ds, valid_ds, bs), c)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LIM65IbkYR8N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#export\n",
        "def get_model(data, lr=0.5, nh=50):\n",
        "    m = data.train_ds.x.shape[1]\n",
        "    model = nn.Sequential(nn.Linear(m,nh), nn.ReLU(), nn.Linear(nh,data.c))\n",
        "    return model, optim.SGD(model.parameters(), lr=lr)\n",
        "\n",
        "class Learner():\n",
        "    def __init__(self, model, opt, loss_func, data):\n",
        "        self.model,self.opt,self.loss_func,self.data = model,opt,loss_func,data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jXn7pmhzYgeW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "learn = Learner(*get_model(data), loss_func, data)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NmiltzxncZgH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def fit(epochs, learn):\n",
        "    for epoch in range(epochs):\n",
        "        learn.model.train()\n",
        "        for xb,yb in learn.data.train_dl:\n",
        "            loss = learn.loss_func(learn.model(xb), yb)\n",
        "            loss.backward()\n",
        "            learn.opt.step()\n",
        "            learn.opt.zero_grad()\n",
        "\n",
        "        learn.model.eval()\n",
        "        with torch.no_grad():\n",
        "            tot_loss,tot_acc = 0.,0.\n",
        "            for xb,yb in learn.data.valid_dl:\n",
        "                pred = learn.model(xb)\n",
        "                tot_loss += learn.loss_func(pred, yb)\n",
        "                tot_acc  += accuracy (pred,yb)\n",
        "        nv = len(learn.data.valid_dl)\n",
        "        print(epoch, tot_loss/nv, tot_acc/nv)\n",
        "    return tot_loss/nv, tot_acc/nv"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PJwuEnF8ccwG",
        "colab_type": "code",
        "outputId": "ff2754ff-3b7e-4057-c3a8-5213901c9cb8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "loss,acc = fit(1, learn)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 tensor(0.3771) tensor(0.8727)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mXZhYkpgcjXb",
        "colab_type": "text"
      },
      "source": [
        "# CallbackHandler\n",
        "\n",
        "This was our training loop (without validation) from the previous notebook, with the inner loop contents factored out:\n",
        "\n",
        "\n",
        "```\n",
        "def one_batch(xb,yb):\n",
        "    pred = model(xb)\n",
        "    loss = loss_func(pred, yb)\n",
        "    loss.backward()\n",
        "    opt.step()\n",
        "    opt.zero_grad()\n",
        "\n",
        "def fit():\n",
        "    for epoch in range(epochs):\n",
        "        for b in train_dl: one_batch(*b)\n",
        "```\n",
        "\n",
        "Add callbacks so we can remove complexity from loop, and make it flexible:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R8CLNBXkcgQn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def one_batch(xb, yb, cb):\n",
        "    if not cb.begin_batch(xb,yb): return\n",
        "    loss = cb.learn.loss_func(cb.learn.model(xb), yb)\n",
        "    if not cb.after_loss(loss): return\n",
        "    loss.backward()\n",
        "    if cb.after_backward(): cb.learn.opt.step()\n",
        "    if cb.after_step(): cb.learn.opt.zero_grad()\n",
        "\n",
        "def all_batches(dl, cb):\n",
        "    for xb,yb in dl:\n",
        "        one_batch(xb, yb, cb)\n",
        "        if cb.do_stop(): return\n",
        "\n",
        "def fit(epochs, learn, cb):\n",
        "    if not cb.begin_fit(learn): return\n",
        "    for epoch in range(epochs):\n",
        "        if not cb.begin_epoch(epoch): continue\n",
        "        all_batches(learn.data.train_dl, cb)\n",
        "        \n",
        "        if cb.begin_validate():\n",
        "            with torch.no_grad(): all_batches(learn.data.valid_dl, cb)\n",
        "        if cb.do_stop() or not cb.after_epoch(): break\n",
        "    cb.after_fit()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e_G0coC2RqJi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Callback():\n",
        "    def begin_fit(self, learn):\n",
        "        self.learn = learn\n",
        "        return True\n",
        "    def after_fit(self): return True\n",
        "    def begin_epoch(self, epoch):\n",
        "        self.epoch=epoch\n",
        "        return True\n",
        "    def begin_validate(self): return True\n",
        "    def after_epoch(self): return True\n",
        "    def begin_batch(self, xb, yb):\n",
        "        self.xb,self.yb = xb,yb\n",
        "        return True\n",
        "    def after_loss(self, loss):\n",
        "        self.loss = loss\n",
        "        return True\n",
        "    def after_backward(self): return True\n",
        "    def after_step(self): return True"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rv0bXK8PRssc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}